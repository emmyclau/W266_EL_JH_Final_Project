{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from apyori import apriori\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from collections import Counter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import TripAdvisor dataset (currently one csv file from each city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247637, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = ['reviews/Chicago_Illinois_1.csv', 'reviews/Las_Vegas_Nevada_1.csv', \\\n",
    "            'reviews/New_York_City_New_York_1.csv', 'reviews/San_Francisco_California_1.csv']\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for path in filepath:\n",
    "    df = pd.read_csv(path)\n",
    "    df['city'] = path.split('/')[1]\n",
    "    data = data.append(df)\n",
    "data.shape #247,637 rows, 19 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.isnull().sum()\n",
    "del data['neighborhood'] #remove column given 80+% null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure every sentence ends in a period and every review is split into sentences\n",
    "#for cases such as {961, 869, 871, 809, 717, 494, 911, 720, 818, 340, 634, 310, 442, 990}\n",
    "#data['review_body'] = data['review_body']+'.'\n",
    "data['review_body'] = [x.split('.') for x in data['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2476"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "data_sample = data.sample(frac=0.01, replace=False,random_state=1)\n",
    "len(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import LIWC scores for each csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247637, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_LIWC = ['reviews/LIWC_scores_Chicago_Illinois_1.csv', 'reviews/LIWC_scores_Las_Vegas_Nevada_1.csv', \\\n",
    "            'reviews/LIWC_scores_New_York_City_New_York_1.csv', 'reviews/LIWC_scores_San_Francisco_California_1.csv']\n",
    "consciousness = pd.DataFrame()\n",
    "for path in filepath_LIWC:\n",
    "    d = pd.read_csv(path)\n",
    "    d['city'] = path.split('/')[1]\n",
    "    consciousness = consciousness.append(d)\n",
    "\n",
    "consciousness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affect</th>\n",
       "      <th>anger</th>\n",
       "      <th>anx</th>\n",
       "      <th>cause</th>\n",
       "      <th>certain</th>\n",
       "      <th>city</th>\n",
       "      <th>cogproc</th>\n",
       "      <th>compare</th>\n",
       "      <th>differ</th>\n",
       "      <th>discrep</th>\n",
       "      <th>...</th>\n",
       "      <th>negemo</th>\n",
       "      <th>percept</th>\n",
       "      <th>posemo</th>\n",
       "      <th>quant</th>\n",
       "      <th>relativ</th>\n",
       "      <th>sad</th>\n",
       "      <th>see</th>\n",
       "      <th>space</th>\n",
       "      <th>tentat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>7.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>LIWC_scores_Chicago_Illinois_1.csv</td>\n",
       "      <td>10.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.48</td>\n",
       "      <td>7.46</td>\n",
       "      <td>5.97</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.99</td>\n",
       "      <td>5.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>5.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.98</td>\n",
       "      <td>LIWC_scores_New_York_City_New_York_1.csv</td>\n",
       "      <td>8.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13976</th>\n",
       "      <td>10.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.51</td>\n",
       "      <td>LIWC_scores_Las_Vegas_Nevada_1.csv</td>\n",
       "      <td>3.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20253</th>\n",
       "      <td>11.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.27</td>\n",
       "      <td>LIWC_scores_New_York_City_New_York_1.csv</td>\n",
       "      <td>5.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.53</td>\n",
       "      <td>10.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>8.23</td>\n",
       "      <td>1.27</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52744</th>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>LIWC_scores_Las_Vegas_Nevada_1.csv</td>\n",
       "      <td>2.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.79</td>\n",
       "      <td>9.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       affect  anger  anx  cause  certain  \\\n",
       "7472     7.46    0.0  0.0   1.49     0.00   \n",
       "3833     5.94    0.0  0.0   0.99     1.98   \n",
       "13976   10.53    0.0  0.0   0.00     3.51   \n",
       "20253   11.39    0.0  0.0   0.63     1.27   \n",
       "52744    9.00    0.0  0.0   0.95     0.95   \n",
       "\n",
       "                                           city  cogproc  compare  differ  \\\n",
       "7472         LIWC_scores_Chicago_Illinois_1.csv    10.45     2.99    2.99   \n",
       "3833   LIWC_scores_New_York_City_New_York_1.csv     8.91      NaN    2.97   \n",
       "13976        LIWC_scores_Las_Vegas_Nevada_1.csv     3.51      NaN    0.00   \n",
       "20253  LIWC_scores_New_York_City_New_York_1.csv     5.70      NaN    1.27   \n",
       "52744        LIWC_scores_Las_Vegas_Nevada_1.csv     2.84      NaN    0.47   \n",
       "\n",
       "       discrep  ...   negemo  percept  posemo  quant  relativ   sad   see  \\\n",
       "7472      2.99  ...     0.00     4.48    7.46   5.97     5.97  0.00  2.99   \n",
       "3833      1.98  ...     0.99     3.96    2.97    NaN    18.81  0.00  1.98   \n",
       "13976     0.00  ...     0.00     0.00   10.53    NaN    14.04  0.00  0.00   \n",
       "20253     0.00  ...     0.63     2.53   10.76    NaN    15.19  0.63  0.63   \n",
       "52744     0.00  ...     0.00     3.79    9.00    NaN    17.06  0.00  3.32   \n",
       "\n",
       "       space  tentat  time  \n",
       "7472    5.97    0.00  0.00  \n",
       "3833    9.90    0.99  7.92  \n",
       "13976   7.02    0.00  5.26  \n",
       "20253   8.23    1.27  6.33  \n",
       "52744   9.95    0.47  5.21  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop redundant columns from dataframe\n",
    "consciousness.drop(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "                   'R'], axis=1, inplace=True)\n",
    "consciousness_sample = consciousness.sample(frac=0.1, replace=False,random_state=1)\n",
    "consciousness_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in NRC Emotion Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('NRC-Emotion-Lexicon-Wordlevel-v0.92.csv', 'r') as f:\n",
    "    df = pd.read_csv(f, delimiter='\\t', header=0, names=['word', 'emotion', 'relation'])\n",
    "\n",
    "# Create lists of emotion words\n",
    "#(anger, fear, anticipation, trust, surprise, sadness, joy, and disgust)\n",
    "anticipation = list(df[(df['emotion']=='anticipation') & (df['relation']!=0)]['word'])\n",
    "anger = list(df[(df['emotion']=='anger') & (df['relation']!=0)]['word'])\n",
    "fear = list(df[(df['emotion']=='fear') & (df['relation']!=0)]['word'])\n",
    "trust = list(df[(df['emotion']=='trust') & (df['relation']!=0)]['word'])\n",
    "surprise = list(df[(df['emotion']=='surprise') & (df['relation']!=0)]['word'])\n",
    "sadness = list(df[(df['emotion']=='sadness') & (df['relation']!=0)]['word'])\n",
    "joy = list(df[(df['emotion']=='joy') & (df['relation']!=0)]['word'])\n",
    "disgust = list(df[(df['emotion']=='disgust') & (df['relation']!=0)]['word'])\n",
    "neg_NRC = list(df[(df['emotion']=='negative') & (df['relation']!=0)]['word'])\n",
    "pos_NRC = list(df[(df['emotion']=='positive') & (df['relation']!=0)]['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and tag each sentence in review with part of speech tags.\n",
    "all_sent = []\n",
    "for item in data_sample['review_body']:\n",
    "    tagged_sent = []\n",
    "    for sent in item:\n",
    "        tokenized = nltk.word_tokenize(sent)\n",
    "        tagged=nltk.pos_tag(tokenized)\n",
    "        tagged_sent.append(tagged)\n",
    "    all_sent.append(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Feature details: https://docs.google.com/document/d/1xbN-xf9eeJsqsfqzynQt1KcV51cy0TLiaX3lAP8evBE/edit?usp=sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "sentences = []\n",
    "# Initiate empty list for subjectivity features\n",
    "nouns = []\n",
    "adj = []\n",
    "comparatives = []\n",
    "superlatives = []\n",
    "adverbs = []\n",
    "word_count = []\n",
    "ne = []\n",
    "tense = []\n",
    "polarity = []\n",
    "dt = []\n",
    "\n",
    "# Initiate empty list for trauma narrative features\n",
    "past_tense = []\n",
    "first_person_plural = []\n",
    "third_person_prn=[]\n",
    "negative_words = []\n",
    "positive_words = []\n",
    "add_narrative_feat = []\n",
    "narr_words=[]\n",
    "# Initiate empty list for TextBlob scores\n",
    "subjectivity = []\n",
    "polarity2 = []\n",
    "\n",
    "# Initiate empty list for NRC Emotion / Plutchikâ€™s scale of emotion\n",
    "tot_anger = []\n",
    "tot_fear = []\n",
    "tot_anticipation = []\n",
    "tot_trust = []\n",
    "tot_surprise = []\n",
    "tot_sadness = []\n",
    "tot_joy = []\n",
    "tot_disgust = []\n",
    "tot_neg_NRC = []\n",
    "tot_pos_NRC = []\n",
    "word_ant = []\n",
    "word_anger = []\n",
    "word_fear = []\n",
    "word_trust = []\n",
    "word_surprise = []\n",
    "word_sad = []\n",
    "word_joy = []\n",
    "word_disgust = []\n",
    "\n",
    "untagged_sent = []\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476\n"
     ]
    }
   ],
   "source": [
    "for rev in all_sent:\n",
    "    review_no = data_sample.index.values[count]\n",
    "    count+=1\n",
    "    #print(rev)\n",
    "    for sent in rev:\n",
    "        if len(sent)==0:\n",
    "            pass\n",
    "        else:\n",
    "            #print(sent)\n",
    "            reviews.append(review_no)\n",
    "            # Append sentence\n",
    "            sentences.append(sent)\n",
    "            # Get word count\n",
    "            word_count.append(len(sent))\n",
    "\n",
    "            counts = Counter(x[1] for x in sent)\n",
    "            total_nouns = counts['NN'] + counts['NNS'] + counts['NNP'] + counts['NNPS']\n",
    "            nouns.append(total_nouns)\n",
    "\n",
    "            #print('total nouns: ' + str(total_nouns))\n",
    "\n",
    "            total_adj = counts['JJ'] + counts['JJR'] + counts['JJS']\n",
    "            adj.append(total_adj)\n",
    "            #print('total adj: ' + str(total_adj))\n",
    "\n",
    "            total_comparatives = counts['JJR']\n",
    "            comparatives.append(total_comparatives)\n",
    "            #print('total comparatives: ' + str(total_comparatives))\n",
    "\n",
    "            total_superlatives = counts['JJS']\n",
    "            superlatives.append(total_superlatives)\n",
    "            #print('total superlatives: ' + str(total_superlatives))\n",
    "\n",
    "            total_adv = counts['RB']+ counts['RBR'] + counts['RBS']\n",
    "            adverbs.append(total_adv)\n",
    "            #print('total adv: ' + str(total_adv))\n",
    "            \n",
    "            # Get named entities\n",
    "            ne_tree =  nltk.ne_chunk(sent)\n",
    "            ne_list = list(ne_tree)\n",
    "            counts_ne = Counter(list(ne_list[i])[0][1] for i in range(len(ne_list)) \\\n",
    "                                if type(ne_list[i]) is nltk.tree.Tree)\n",
    "            total_ne = counts_ne['NNP']\n",
    "            ne.append(total_ne)\n",
    "            \n",
    "            count_future = counts['MD']\n",
    "            #print('future words: ' + str(count_future))\n",
    "            count_present = counts['VBP'] + counts['VBG'] + counts['VBZ']\n",
    "            #print('present words: ' + str(count_present))\n",
    "            count_past = counts['VBD'] + counts ['VBN']\n",
    "            past_tense.append(count_past)\n",
    "            #print('past words: ' + str(count_past))\n",
    "            # Get max count from future, present, and past, and use as tense of sentence.\n",
    "            tense_count = max(count_future, count_present, count_past)\n",
    "            #print('max: ' + str(tense_count))\n",
    "            if tense_count == count_future:\n",
    "                tense.append('future')\n",
    "                #print('this sentence is written in future tense')\n",
    "            elif tense_count == count_present:\n",
    "                tense.append('present')\n",
    "                #print('this sentence is written in present tense')\n",
    "            else:\n",
    "                tense.append('past')\n",
    "                #print('this sentence is written in past tense')\n",
    "            \n",
    "            # CD represents cardinal numbers\n",
    "            count_dt = counts['CD']\n",
    "            dt.append(count_dt)\n",
    "            #print('total digits: ' + str(count_dt))\n",
    "            \n",
    "            count_fp = Counter(x[0] for x in sent if x[0] in ['we','us','our'])\n",
    "            first_person_plural.append(sum(count_fp.values()))\n",
    "            #print('total first person pronoun words: ' + str(sum(count_fp.values())))\n",
    "            \n",
    "            count_tp = Counter(x[0] for x in sent if x[0] in ['he','she','it','him','her','his','hers','its','they',\\\n",
    "                                                              'them','their','theirs'])            \n",
    "            third_person_prn.append(sum(count_tp.values()))\n",
    "            \n",
    "            count_add_narr = Counter(x[0] for x in sent if x[0] in ['say','tell','said','told','saying','telling',\\\n",
    "                                                                   'then', 'after', 'before','initially', 'first',\\\n",
    "                                                                   'next', 'while', 'during', 'finally', 'eventually',\\\n",
    "                                                                   'end', 'start'])\n",
    "            add_narrative_feat.append(sum(count_add_narr.values()))\n",
    "            narr_words.append(count_add_narr.keys()) \n",
    "            # Determine polarity of sentence. Count the number of words from SentiWordNet \n",
    "            # (having either nonzero positive polarity score or nonzero negative polarity score) \n",
    "            # present in a sentence\n",
    "            sent_pos = 0\n",
    "            sent_neg = 0\n",
    "            for word in sent:\n",
    "                #print(word)\n",
    "                pos_list = []\n",
    "                neg_list = []\n",
    "                # Get word synonyms\n",
    "                syns = wordnet.synsets(word[0])\n",
    "                if syns == []:\n",
    "                    pass\n",
    "                else:\n",
    "                    # Append all polarity scores for each word and take the max positive and negative\n",
    "                    # to use as overall word polarity. \n",
    "                    for s in syns:\n",
    "                        #print(s.name)\n",
    "                        swn_synset =swn.senti_synset(s.name())\n",
    "                        pos_list.append(swn_synset.pos_score())\n",
    "                        neg_list.append(swn_synset.neg_score())\n",
    "                        #print('Positive: ' + str(pos_list))\n",
    "                        #print('Negative: ' + str(neg_list))\n",
    "                    word_pos = max(pos_list)\n",
    "                    #print('Positive max: ' + str(word_pos))\n",
    "                    word_neg = max(neg_list)\n",
    "                    #print('Negative max: ' + str(word_neg))\n",
    "                    if word_pos > word_neg:\n",
    "                        sent_pos +=1\n",
    "                    if word_neg > word_pos:\n",
    "                        sent_neg +=1\n",
    "            negative_words.append(sent_neg)\n",
    "            positive_words.append(sent_pos)\n",
    "            # Sum up number of positive and negative words in sentence\n",
    "            total_polarity = sent_pos+sent_neg\n",
    "            polarity.append(total_polarity)\n",
    "            \n",
    "            # NRC Emotions: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust\n",
    "            count_ant = Counter(x[0] for x in sent if x[0] in anticipation)\n",
    "            tot_anticipation.append(sum(count_ant.values()))\n",
    "            word_ant.append(count_ant.keys())\n",
    "            count_anger = Counter(x[0] for x in sent if x[0] in anger)\n",
    "            tot_anger.append(sum(count_anger.values()))\n",
    "            word_anger.append(count_anger.keys())\n",
    "            count_fear = Counter(x[0] for x in sent if x[0] in fear)\n",
    "            tot_fear.append(sum(count_fear.values()))\n",
    "            word_fear.append(count_fear.keys())\n",
    "            count_trust = Counter(x[0] for x in sent if x[0] in trust)\n",
    "            tot_trust.append(sum(count_trust.values()))\n",
    "            word_trust.append(count_trust.keys())\n",
    "            count_surprise = Counter(x[0] for x in sent if x[0] in surprise)\n",
    "            tot_surprise.append(sum(count_surprise.values()))\n",
    "            word_surprise.append(count_surprise.keys())\n",
    "            count_sad = Counter(x[0] for x in sent if x[0] in sadness)\n",
    "            tot_sadness.append(sum(count_sad.values()))\n",
    "            word_sad.append(count_sad.keys())\n",
    "            count_joy = Counter(x[0] for x in sent if x[0] in joy)\n",
    "            tot_joy.append(sum(count_joy.values()))\n",
    "            word_joy.append(count_joy.keys())\n",
    "            count_disgust = Counter(x[0] for x in sent if x[0] in disgust)\n",
    "            tot_disgust.append(sum(count_disgust.values()))\n",
    "            word_disgust.append(count_disgust.keys())\n",
    "            count_neg = Counter(x[0] for x in sent if x[0] in neg_NRC)\n",
    "            tot_neg_NRC.append(sum(count_neg.values()))\n",
    "            count_pos = Counter(x[0] for x in sent if x[0] in pos_NRC)\n",
    "            tot_pos_NRC.append(sum(count_pos.values()))\n",
    "            \n",
    "            # Use TextBlob to sentence-level subjectivity and polarity scores\n",
    "            # Each word in the lexicon has scores for:\n",
    "            # 1) polarity: negative vs. positive    (-1.0 => +1.0)\n",
    "            # 2) subjectivity: objective vs. subjective (+0.0 => +1.0)\n",
    "            new_sent = \" \".join([a for a, b in sent])\n",
    "            text = TextBlob(new_sent)\n",
    "            untagged_sent.append(new_sent)\n",
    "            subjectivity.append(text.sentiment.subjectivity)\n",
    "            polarity2.append(text.sentiment.polarity)\n",
    "            \n",
    "            \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of sentence-level records\n",
    "setup = {'Reviews':reviews, 'Sentence':sentences, 'Untagged': untagged_sent, \\\n",
    "         'Word_Count': word_count, 'Nouns': nouns, 'Adjectives':adj, \\\n",
    "         'Comparatives': comparatives, 'Superlatives':superlatives, 'Adverbs': adverbs,\\\n",
    "         'Subjectivity': subjectivity, 'Tense': tense, 'Digits': dt, \\\n",
    "         'Polarity': polarity, 'Polarity2': polarity2, 'First Person': first_person_plural,     \n",
    "         'Named Entities': ne, 'Third Person': third_person_prn, 'Past Tense': past_tense, \\\n",
    "         'Narrative Seq': add_narrative_feat, 'Narr Words': narr_words, 'Neg_Words': negative_words, 'Anger':tot_anger, \\\n",
    "         'Anger Words': word_anger, 'Anticipation':tot_anticipation, 'Ant Words': word_ant, 'Fear':tot_fear,\\\n",
    "         'Fear Words': word_fear, 'Trust': tot_trust, 'Trust Words': word_trust, 'Surprise':tot_surprise, \\\n",
    "         'Surprise Words': word_surprise, 'Sad': tot_sadness, 'Sad Words': word_sad, 'Joy':tot_joy, \\\n",
    "         'Joy Words': word_joy,'Disgust': tot_disgust, 'Disgust Words': word_disgust, 'neg_NRC':tot_neg_NRC, \\\n",
    "         'pos_NRC':tot_pos_NRC\n",
    "}\n",
    "opinions = pd.DataFrame(setup)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opinions.to_csv('opinions_NRC_sample', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opinions = pd.read_csv('opinions_NRC_sample')\n",
    "#opinions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opinions = opinions.drop_duplicates(subset=['Reviews'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate sentence-level dataframe back into review-level\n",
    "aggregation_functions = {'Adjectives': 'sum', 'Adverbs': 'sum', 'Comparatives': 'sum', 'Superlatives': 'sum',\n",
    "                        'Digits': 'sum', 'Nouns': 'sum', 'Polarity': 'sum', 'Polarity2': 'mean',\n",
    "                        'Subjectivity': 'mean', 'Tense': lambda col: ', '.join(col), 'Word_Count': 'sum',\n",
    "                        'First Person': 'sum', 'Named Entities': 'sum', 'Third Person': 'sum', 'Past Tense': 'sum',\n",
    "                        'Narrative Seq': 'sum', 'Narr Words': 'sum','Neg_Words': 'sum', 'Anger':'sum', 'Anger Words': 'sum',\n",
    "                         'Anticipation':'sum', 'Ant Words': 'sum', 'Fear':'sum', 'Fear Words': 'sum', \n",
    "                         'Trust': 'sum', 'Trust Words': 'sum', 'Surprise':'sum', 'Surprise Words': 'sum',\n",
    "                         'Sad': 'sum', 'Sad Words': 'sum', 'Joy': 'sum', 'Joy Words': 'sum', 'Disgust': 'sum', \n",
    "                         'Disgust Words': 'sum', 'neg_NRC':'sum', 'pos_NRC':'sum'}\n",
    "df1 = opinions.groupby(opinions['Reviews']).agg(aggregation_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = data_sample[~data_sample.index.duplicated(keep='first')]\n",
    "consciousness_sample = consciousness_sample[~consciousness_sample.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2438"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2438"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make values percentages\n",
    "df2 = df1[['Adjectives','Adverbs', 'Comparatives', 'Superlatives', 'Digits', 'Nouns', 'Polarity',\n",
    "           'Named Entities', 'First Person', 'Third Person', 'Past Tense', 'Neg_Words', 'Narrative Seq', \n",
    "           'Anger', 'Anticipation', 'Fear', 'Trust', 'Surprise', 'Sad', 'Joy', 'Disgust',\n",
    "           'neg_NRC', 'pos_NRC']].div(df1.Word_Count, axis=0)\n",
    "df2['Tense'] = df1['Tense']\n",
    "df2['Polarity2'] = df1['Polarity2']\n",
    "df2['Subjectivity'] = df1['Subjectivity']\n",
    "df2['Word_Count'] = df1['Word_Count']\n",
    "df2['Ant Words'] = df1['Ant Words']\n",
    "df2['Anger Words'] = df1['Anger Words']\n",
    "df2['Fear Words'] = df1['Fear Words']\n",
    "df2['Trust Words'] = df1['Trust Words']\n",
    "df2['Surprise Words'] = df1['Surprise Words']\n",
    "df2['Sad Words'] = df1['Sad Words']\n",
    "df2['Joy Words'] = df1['Joy Words']\n",
    "df2['Disgust Words'] = df1['Disgust Words']\n",
    "df2['Narr Words'] = df1['Narr Words']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column for Narrative to denote presence of narrative (if sum of % of\n",
    "# past tense, third person pronoun and narrative feature variables > 0.5, then 1 for narrative)\n",
    "df2['Narrative'] = [1 if (df2.iloc[i]['Past Tense']+df2.iloc[i]['Third Person']+df2.iloc[i]['Narrative Seq'])>=0.5\n",
    "                    else 0 for i in range(len(df2))]\n",
    "df2['Tense'] = [Counter(i.split(', ')).most_common()[0][0] for i in df2['Tense']]\n",
    "df2['affect'] = consciousness_sample['affect']*.01\n",
    "df2['posemo'] = consciousness_sample['posemo']*.01\n",
    "df2['negemo'] = consciousness_sample['negemo']*.01\n",
    "df2['Trauma'] = [1 if (df2.iloc[i]['Narrative']>0) and (df2.iloc[i]['Neg_Words']>0) \n",
    "                 and (df2.iloc[i]['First Person']>0) else 0 for i in range(len(df2))]\n",
    "df2['anx'] = consciousness_sample['anx']*.01*.01\n",
    "df2['anger'] = consciousness_sample['anger']*.01\n",
    "df2['sad'] = consciousness_sample['sad']*.01 #[:8088]\n",
    "df2['cogproc'] = consciousness_sample['cogproc']*.01 #[:8088]\n",
    "df2['insight'] = consciousness_sample['insight']*.01 #[:8088]\n",
    "df2['cause'] = consciousness_sample['cause']*.01 #[:8088]\n",
    "df2['discrep'] = consciousness_sample['discrep']*.01 #[:8088]\n",
    "df2['tentat'] = consciousness_sample['tentat']*.01 #[:8088]\n",
    "df2['certain'] = consciousness_sample['certain']*.01 #[:8088]\n",
    "df2['differ'] = consciousness_sample['differ']*.01 #[:8088]\n",
    "df2['Rating'] = data_sample['rating'] #[:8088]\n",
    "df2['Helpful'] = data_sample['helpful_vote'] #[:8088]\n",
    "df2['city'] = data_sample['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2164, 54)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with rating of 3\n",
    "df2 = df2[df2.Rating != 30]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['Rating'] = df2['Rating'].map({50: 1, 40: 1, 20: 0, 10: 0})\n",
    "df2['Tense'] = df2['Tense'].map({'past': -1, 'present': 2, 'future': 3})\n",
    "df2['city'] = df2['city'].map({'Chicago_Illinois_1.csv': 1, 'New_York_City_New_York_1.csv': 2, \n",
    "                              'San_Francisco_California_1.csv': 3, 'Las_Vegas_Nevada_1.csv': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create spatial and temporal columns as stated in What Happens in Vegas\n",
    "\n",
    "consciousness_sample['spatial'] = [2 if (consciousness_sample.iloc[s]['space'] > 0) & (consciousness_sample.iloc[s]['percept'] > 0) \n",
    "                            else 1 if (consciousness_sample.iloc[s]['space'] > 0) & (consciousness_sample.iloc[s]['percept'] == 0) \n",
    "                            else 0 for s in range(len(consciousness_sample))]\n",
    "consciousness_sample['temporal'] = [2 if (consciousness_sample.iloc[s]['time'] > 0) & (consciousness_sample.iloc[s]['cause'] > 0) \n",
    "                            else 1 if (consciousness_sample.iloc[s]['time'] > 0) | (consciousness_sample.iloc[s]['cause'] > 0) \n",
    "                            else 0 for s in range(len(consciousness_sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['spatial'] = consciousness_sample['spatial']\n",
    "df2['temporal'] = consciousness_sample['temporal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 1883\n",
      "Number of negative reviews: 281\n",
      "% of positive: 87.01\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive reviews: ' + str(len(df2[df2['Rating'] == 1])))\n",
    "print('Number of negative reviews: ' + str(len(df2[df2['Rating'] == 0])))\n",
    "print('% of positive: ' + str(round((len(df2[df2['Rating'] == 1])/(len(df2[df2['Rating'] == 1])+len(df2[df2['Rating'] == 0])))*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago reviews: 536\n",
      "NYC reviews: 534\n",
      "SF reviews: 534\n",
      "Las Vegas reviews: 560\n"
     ]
    }
   ],
   "source": [
    "print('Chicago reviews: ' + str(len(df2[df2['city'] == 1])))\n",
    "print('NYC reviews: ' + str(len(df2[df2['city'] == 2])))\n",
    "print('SF reviews: ' + str(len(df2[df2['city'] == 3])))\n",
    "print('Las Vegas reviews: ' + str(len(df2[df2['city'] == 4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv('Chi_NYC_LV_features_NRC', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df2['Rating']\n",
    "#'Adjectives', 'Adverbs', 'Comparatives', 'Superlatives', 'Digits',\n",
    "       #'Nouns', 'Polarity', 'Named Entities', 'Tense', 'Past Tense', 'Neg_Words', 'Narrative', \n",
    "    # 'posemo', 'negemo','Trauma', 'anx', 'anger', 'sad','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ' \n",
    "X = df2[['Polarity2','Subjectivity', 'Word_Count', 'Named Entities',\n",
    "       'affect', 'cogproc','Helpful', 'city', 'Trauma','spatial', 'temporal', 'Third Person', 'Past Tense']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.89\n",
      "[[ 16  68]\n",
      " [  6 560]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.19      0.30        84\n",
      "          1       0.89      0.99      0.94       566\n",
      "\n",
      "avg / total       0.87      0.89      0.86       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = df2['Rating']\n",
    "#'Adjectives', 'Adverbs', 'Comparatives', 'Superlatives', 'Digits',\n",
    "       #'Nouns', 'Polarity', 'Named Entities', 'Tense', 'Past Tense', 'Neg_Words', 'Narrative', \n",
    "    # 'posemo', 'negemo','Trauma', 'anx', 'anger', 'sad','insight', 'cause', 'discrep', 'tentat', 'certain', 'differ' \n",
    "X1 = df2[['Polarity2','Subjectivity', 'Word_Count', 'Named Entities',\n",
    "       'affect', 'cogproc','Helpful', 'city', 'Trauma','spatial', 'temporal', 'Third Person', 'Past Tense', 'Anger', 'Anticipation', 'Fear',\\\n",
    "       'Trust', 'Surprise', 'Sad', 'Joy','Disgust']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.89\n",
      "[[ 16  68]\n",
      " [  6 560]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.19      0.30        84\n",
      "          1       0.89      0.99      0.94       566\n",
      "\n",
      "avg / total       0.87      0.89      0.86       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, random_state=0, stratify=y)\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train1, y_train1)\n",
    "y_pred1 = logreg1.predict(X_test1)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg1.score(X_test1, y_test1)))\n",
    "confusion_matrix = metrics.confusion_matrix(y_test1, y_pred1)\n",
    "print(confusion_matrix)\n",
    "print(metrics.classification_report(y_test1, y_pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1883\n",
       "0    1883\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "# Separate majority and minority classes\n",
    "df_majority = df2[df2.Rating==1]\n",
    "df_minority = df2[df2.Rating==0]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df2[df2['Rating'] == 1]),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_upsampled.to_csv('Chi_NYC_LV_features_NRC_upsampled', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.76\n",
      "[[438 128]\n",
      " [139 425]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.77      0.77       566\n",
      "          1       0.77      0.75      0.76       564\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Scale data to determine feature importance\n",
    "y2 = df_upsampled.Rating\n",
    "X2 = df_upsampled[['Polarity2','Subjectivity', 'Word_Count', 'Named Entities',\n",
    "       'affect', 'cogproc','Helpful', 'city', 'Trauma','spatial', 'temporal', 'Third Person', 'Past Tense']]\n",
    "#scaler = RobustScaler()\n",
    "#scaler.fit(X1) \n",
    "#X_scaled1 = pd.DataFrame(scaler.transform(X1),columns = X1.columns)\n",
    "#X_scaled1.head()\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_train2, y_train2)\n",
    "y_pred2 = logreg2.predict(X_test2)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg2.score(X_test2, y_test2)))\n",
    "confusion_matrix1 = metrics.confusion_matrix(y_test2, y_pred2)\n",
    "print(confusion_matrix1)\n",
    "print(metrics.classification_report(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.77\n",
      "[[438 128]\n",
      " [132 432]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.77      0.77       566\n",
      "          1       0.77      0.77      0.77       564\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Scale data to determine feature importance\n",
    "y2 = df_upsampled.Rating\n",
    "X2 = df_upsampled[['Polarity2','Subjectivity', 'Word_Count', 'Named Entities',\n",
    "       'affect', 'cogproc','Helpful', 'city', 'Trauma','spatial', 'temporal', 'Third Person', 'Past Tense', 'Anger', 'Anticipation', 'Fear',\\\n",
    "       'Trust', 'Surprise', 'Sad', 'Joy','Disgust']]\n",
    "#scaler = RobustScaler()\n",
    "#scaler.fit(X1) \n",
    "#X_scaled1 = pd.DataFrame(scaler.transform(X1),columns = X1.columns)\n",
    "#X_scaled1.head()\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=0)\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_train2, y_train2)\n",
    "y_pred2 = logreg2.predict(X_test2)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg2.score(X_test2, y_test2)))\n",
    "confusion_matrix1 = metrics.confusion_matrix(y_test2, y_pred2)\n",
    "print(confusion_matrix1)\n",
    "print(metrics.classification_report(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Incorrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "results_df = X_test2\n",
    "results_df['y_test']= y_test2\n",
    "results_df['y_pred']=y_pred2\n",
    "incorrects = results_df[results_df['y_test']!=results_df['y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample2 = data.sample(frac=0.01, replace=False,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "incorrects['review_body']=np.nan\n",
    "incorrects['Anger Words']=np.nan\n",
    "incorrects['Ant Words'] =np.nan\n",
    "incorrects['Fear Words']=np.nan\n",
    "incorrects['Trust Words']=np.nan\n",
    "incorrects['Surprise Words'] =np.nan\n",
    "incorrects['Joy Words'] =np.nan\n",
    "incorrects['Disgust Words'] =np.nan\n",
    "incorrects['Sad Words'] =np.nan\n",
    "\n",
    "for i in incorrects.index.values:\n",
    "    incorrects['review_body'].loc[i] = str(data_sample2['review_body'].loc[i])\n",
    "    incorrects['Anger Words'].loc[i] = str(df2['Anger Words'].loc[i])\n",
    "    incorrects['Ant Words'].loc[i] = str(df2['Ant Words'].loc[i])\n",
    "    incorrects['Fear Words'].loc[i] = str(df2['Fear Words'].loc[i])\n",
    "    incorrects['Trust Words'].loc[i] = str(df2['Trust Words'].loc[i])\n",
    "    incorrects['Surprise Words'].loc[i] = str(df2['Surprise Words'].loc[i])\n",
    "    incorrects['Joy Words'].loc[i] = str(df2['Joy Words'].loc[i])\n",
    "    incorrects['Disgust Words'].loc[i] = str(df2['Disgust Words'].loc[i])\n",
    "    incorrects['Sad Words'].loc[i] = str(df2['Sad Words'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/arthur/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "results_df['review_body']=np.nan\n",
    "results_df['Anger Words']=np.nan\n",
    "results_df['Ant Words'] =np.nan\n",
    "results_df['Fear Words']=np.nan\n",
    "results_df['Trust Words']=np.nan\n",
    "results_df['Surprise Words'] =np.nan\n",
    "results_df['Joy Words'] =np.nan\n",
    "results_df['Disgust Words'] =np.nan\n",
    "results_df['Sad Words'] =np.nan\n",
    "for i in results_df.index.values:\n",
    "    results_df['review_body'].loc[i] = str(data_sample2['review_body'].loc[i])\n",
    "    results_df['Anger Words'].loc[i] = str(df2['Anger Words'].loc[i])\n",
    "    results_df['Ant Words'].loc[i] = str(df2['Ant Words'].loc[i])\n",
    "    results_df['Fear Words'].loc[i] = str(df2['Fear Words'].loc[i])\n",
    "    results_df['Trust Words'].loc[i] = str(df2['Trust Words'].loc[i])\n",
    "    results_df['Surprise Words'].loc[i] = str(df2['Surprise Words'].loc[i])\n",
    "    results_df['Joy Words'].loc[i] = str(df2['Joy Words'].loc[i])\n",
    "    results_df['Disgust Words'].loc[i] = str(df2['Disgust Words'].loc[i])\n",
    "    results_df['Sad Words'].loc[i] = str(df2['Sad Words'].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_positives = incorrects[(incorrects['y_test']==0) & (incorrects['y_pred']==1)]\n",
    "false_negatives = incorrects[(incorrects['y_test']==1) & (incorrects['y_pred']==0)]\n",
    "true_positives = results_df[(results_df['y_test']==1) & (results_df['y_pred']==1)]\n",
    "true_negatives = results_df[(results_df['y_test']==0) & (results_df['y_pred']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_positives.to_csv('false_positives_NRC_upsampled', sep=',')\n",
    "false_negatives.to_csv('false_negatives_NRC_upsampled', sep=',')\n",
    "true_positives.to_csv('true_positives_NRC_upsampled', sep=',')\n",
    "true_negatives.to_csv('true_negatives_NRC_upsampled', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "print(len(true_negatives))\n",
    "true_negatives = true_negatives[~true_negatives.index.duplicated(keep='first')]\n",
    "print(len(true_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(false_positives))\n",
    "false_positives = false_positives[~false_positives.index.duplicated(keep='first')]\n",
    "print(len(false_positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Words in True Negative and Positive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger Words in True Negative Reviews\n",
      "'hot'\n",
      "'disgusting', 'smell'\n",
      "'terrible'\n",
      "'bad', 'treat', 'money'\n",
      "'bad', 'warp'\n",
      "'disappointed', 'noisy'\n",
      "'hot'\n",
      "'noisy'\n",
      "'terrible'\n",
      "'bad'\n",
      "'treat'\n",
      "'force', 'fee'\n",
      "'stolen'\n",
      "'money'\n",
      "'broken', 'stolen', 'cash'\n",
      "'hit', 'fee', 'greed'\n",
      "'awful'\n",
      "'shoddy'\n",
      "'bad', 'terrible'\n",
      "'disappointed'\n",
      "'disappointed'\n",
      "'bad'\n",
      "'misleading'\n",
      "'disappointed'\n",
      "'terrible'\n",
      "'bad'\n",
      "'disappointed'\n",
      "'cutting'\n",
      "'noisy'\n",
      "'horrible'\n",
      "'fee', 'ridiculous'\n",
      "'hot'\n",
      "'horrible', 'money'\n",
      "'bad'\n",
      "'stolen'\n",
      "'money'\n",
      "'noisy', 'bad'\n",
      "'fee'\n",
      "'smell', 'horrible'\n",
      "'horrible'\n",
      "'mad'\n",
      "'ruined'\n",
      "'remove'\n",
      "'bad'\n",
      "'money'\n",
      "'wireless', 'row'\n",
      "'bad'\n",
      "'terrible'\n",
      "Anticipation Words in True Negative Reviews\n",
      "'renovation'\n",
      "'time'\n",
      "'god'\n",
      "'share'\n",
      "'friendly'\n",
      "'treat', 'money'\n",
      "'advance', 'arrive'\n",
      "'responsive'\n",
      "'pay', 'advance'\n",
      "'time'\n",
      "'time'\n",
      "'excited', 'arrive'\n",
      "'spa', 'celebrating', 'time', 'perfect'\n",
      "'time'\n",
      "'honeymoon'\n",
      "'good'\n",
      "'vacation'\n",
      "'treat', 'mother'\n",
      "'pretty'\n",
      "'time'\n",
      "'star'\n",
      "'time'\n",
      "'pay'\n",
      "'money'\n",
      "'cash'\n",
      "'time'\n",
      "'medical'\n",
      "'ready'\n",
      "'share', 'opportunity'\n",
      "'arrival', 'friendly', 'ready'\n",
      "'neighborhood'\n",
      "'top'\n",
      "'coming'\n",
      "'birthday'\n",
      "'excited', 'time', 'finally'\n",
      "'roulette', 'good'\n",
      "'renovation'\n",
      "'frantic'\n",
      "'good'\n",
      "'long'\n",
      "'long'\n",
      "'reconstruction'\n",
      "'time'\n",
      "'money'\n",
      "'money'\n",
      "'result'\n",
      "'friendly'\n",
      "'adventure'\n",
      "'coming', 'arrival'\n",
      "'honeymoon', 'good'\n",
      "'quote'\n",
      "'start'\n",
      "'instructions', 'advance'\n",
      "'daily'\n",
      "'coming'\n",
      "'time'\n",
      "'money'\n",
      "'wireless'\n",
      "Disgust Words in True Negative Reviews\n",
      "'disgusting', 'smell'\n",
      "'terrible'\n",
      "'bad', 'treat'\n",
      "'clumsy'\n",
      "'bad'\n",
      "'fat'\n",
      "'disappointed', 'unclean'\n",
      "'shabby'\n",
      "'terrible'\n",
      "'bad'\n",
      "'treat'\n",
      "'dirty'\n",
      "'greed'\n",
      "'awful'\n",
      "'shoddy'\n",
      "'bad', 'dirty', 'dirt', 'terrible'\n",
      "'weird'\n",
      "'unsatisfactory'\n",
      "'disappointed'\n",
      "'disappointed'\n",
      "'finally'\n",
      "'bad'\n",
      "'misleading'\n",
      "'disappointed'\n",
      "'frantic'\n",
      "'terrible'\n",
      "'bad'\n",
      "'disappointed'\n",
      "'cutting'\n",
      "'horrible'\n",
      "'ridiculous'\n",
      "'dirty', 'spider'\n",
      "'horrible'\n",
      "'unpleasant'\n",
      "'bad'\n",
      "'bad'\n",
      "'dirty'\n",
      "'smell', 'filthy', 'horrible'\n",
      "'dirty', 'horrible'\n",
      "'mad'\n",
      "'ruined'\n",
      "'trash'\n",
      "'shabby'\n",
      "'bad'\n",
      "'bad'\n",
      "'toilet'\n",
      "'terrible'\n",
      "Sad Words in True Negative Reviews\n",
      "'terrible'\n",
      "'unfortunate'\n",
      "'bad', 'treat'\n",
      "'strip'\n",
      "'bad', 'warp'\n",
      "'fat'\n",
      "'sadly', 'disappointed'\n",
      "'negative'\n",
      "'terrible'\n",
      "'bad'\n",
      "'treat', 'mother'\n",
      "'worn'\n",
      "'broken'\n",
      "'strip'\n",
      "'handicap'\n",
      "'awful'\n",
      "'bad', 'terrible'\n",
      "'worn'\n",
      "'disappointed'\n",
      "'disappointed'\n",
      "'bad'\n",
      "'disappointed'\n",
      "'terrible'\n",
      "'bad'\n",
      "'disappointed', 'tax'\n",
      "'cutting'\n",
      "'worse'\n",
      "'late'\n",
      "'unpleasant'\n",
      "'bad'\n",
      "'bad'\n",
      "'refused'\n",
      "'mad'\n",
      "'ruined', 'disabled', 'strip'\n",
      "'remove', 'trash'\n",
      "'bad'\n",
      "'problem'\n",
      "'bad'\n",
      "'worn', 'dark'\n",
      "'disappointing'\n",
      "'terrible'\n"
     ]
    }
   ],
   "source": [
    "#'Anger Words','Ant Words', 'Fear Words', 'Trust Words', 'Surprise Words', 'Joy Words',\n",
    "                #'Disgust Words', 'Sad Words'\n",
    "print('Anger Words in True Negative Reviews')\n",
    "for i in true_negatives.index.values:\n",
    "    if true_negatives.loc[i]['Anger Words']!='dict_keys([])':\n",
    "        print(true_negatives.loc[i]['Anger Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))\n",
    "\n",
    "print('Anticipation Words in True Negative Reviews')\n",
    "for i in true_negatives.index.values:\n",
    "    if true_negatives.loc[i]['Ant Words']!='dict_keys([])':\n",
    "        print(true_negatives.loc[i]['Ant Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))\n",
    "        \n",
    "print('Disgust Words in True Negative Reviews')\n",
    "for i in true_negatives.index.values:\n",
    "    if true_negatives.loc[i]['Disgust Words']!='dict_keys([])':\n",
    "        print(true_negatives.loc[i]['Disgust Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))\n",
    "\n",
    "print('Sad Words in True Negative Reviews')\n",
    "for i in true_negatives.index.values:\n",
    "    if true_negatives.loc[i]['Sad Words']!='dict_keys([])':\n",
    "        print(true_negatives.loc[i]['Sad Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trust Words in True Positive Reviews\n",
      "'attentive'\n",
      "'center'\n",
      "'personal'\n",
      "'good'\n",
      "'recommend'\n",
      "'clean', 'excellent', 'helpful', 'professional', 'fairly'\n",
      "'clean'\n",
      "'attentive'\n",
      "'clean'\n",
      "'fully'\n",
      "'lovely', 'clean'\n",
      "'friendly', 'efficient', 'wonderful'\n",
      "'friend'\n",
      "'perfect', 'money'\n",
      "'pleasant'\n",
      "'friendly'\n",
      "'honest'\n",
      "'wonderful', 'attentive'\n",
      "'center'\n",
      "'heritage'\n",
      "'good'\n",
      "'interior', 'thoughtful', 'friendly'\n",
      "'clean'\n",
      "'recommend'\n",
      "'oasis'\n",
      "'helpful', 'professional'\n",
      "'loyalty', 'perfect'\n",
      "'honest'\n",
      "'excellent'\n",
      "'shopping'\n",
      "'genuine'\n",
      "'good'\n",
      "'honeymoon'\n",
      "'shopping', 'food'\n",
      "'clean', 'friendly', 'helpful'\n",
      "'excellent'\n",
      "'favorite'\n",
      "'excellent'\n",
      "'clean'\n",
      "'perfect', 'friendly'\n",
      "'share'\n",
      "'good'\n",
      "'helpful', 'friendly'\n",
      "'shopping'\n",
      "'perfect', 'friendly'\n",
      "'wonderful'\n",
      "'smile'\n",
      "'good'\n",
      "'excellent'\n",
      "'shopping'\n",
      "'lovely', 'excellent'\n",
      "'clean'\n",
      "'perfect'\n",
      "'clean'\n",
      "'shopping'\n",
      "'perfect'\n",
      "'pretty', 'good'\n",
      "'found', 'excellent'\n",
      "'pleasant', 'friendly', 'helpful'\n",
      "'food'\n",
      "'friendly'\n",
      "'attentive'\n",
      "'excellent', 'money'\n",
      "'friendly'\n",
      "'level'\n",
      "'clean'\n",
      "'friendly'\n",
      "'shopping'\n",
      "'enjoy'\n",
      "'star'\n",
      "'recommend', 'attentive', 'friendly', 'professional'\n",
      "'good', 'magnificent'\n",
      "'helpful', 'good'\n",
      "'friendly'\n",
      "'structure'\n",
      "'clean'\n",
      "'attentive', 'excellent'\n",
      "'friendly'\n",
      "'helpful'\n",
      "'good'\n",
      "'wonderful'\n",
      "'usual', 'comfort'\n",
      "'friendly'\n",
      "'pretty', 'good'\n",
      "'wonderful'\n",
      "'top', 'good', 'related'\n",
      "'important', 'grandchildren', 'food'\n",
      "'top'\n",
      "'good'\n",
      "'friendly'\n",
      "'good'\n",
      "'truth'\n",
      "'found'\n",
      "'perfect'\n",
      "'clean'\n",
      "'deal'\n",
      "'clean'\n",
      "'attendant', 'attentive', 'helpful'\n",
      "'clean', 'good', 'food', 'found'\n",
      "'praise'\n",
      "'good', 'effective'\n",
      "'friendly', 'efficient'\n",
      "'clean'\n",
      "'food'\n",
      "'excellent', 'good'\n",
      "'perfect', 'good', 'helpful'\n",
      "'bartender'\n",
      "'excellent'\n",
      "'good', 'entertainment'\n",
      "'perfect'\n",
      "'wonderful'\n",
      "'pleasant'\n",
      "'good', 'excellent'\n",
      "'clean'\n",
      "'perfect'\n",
      "'hire'\n",
      "'budget', 'good'\n",
      "'clean', 'provide'\n",
      "'found', 'clean'\n",
      "'excellent'\n",
      "'good'\n",
      "'food', 'wonderful'\n",
      "'friend'\n",
      "'pleasant'\n",
      "'clean'\n",
      "'maintenance'\n",
      "'interior'\n",
      "'good', 'deal'\n",
      "'happy', 'level'\n",
      "'clean', 'friendly', 'professional'\n",
      "'deal', 'helpful'\n",
      "'good'\n",
      "'friendly', 'happy'\n",
      "'perfect', 'clean', 'important'\n",
      "'friendly'\n",
      "'pleasant'\n",
      "'recommend'\n",
      "'wonderful', 'base'\n",
      "'enjoy'\n",
      "'perfect'\n",
      "'perfect'\n",
      "'perfect', 'clean', 'honest', 'helpful', 'food'\n",
      "'wonderful', 'helpful'\n",
      "'good'\n",
      "'good'\n",
      "'excellent', 'friendly', 'helpful', 'clean'\n",
      "'center'\n",
      "'good'\n",
      "'friend'\n",
      "'excellent'\n",
      "'magnificent'\n",
      "'green'\n",
      "'clean'\n",
      "'top'\n",
      "'brilliant'\n",
      "'clean'\n",
      "'magnificent'\n",
      "'quaint'\n",
      "'excellent', 'friendly'\n",
      "'deal', 'friendly'\n",
      "'wonderful', 'excellent'\n",
      "'friendly'\n",
      "'perfect'\n",
      "'found'\n",
      "'excited'\n",
      "'recommend', 'magnificent'\n",
      "'good', 'clean'\n",
      "'pay', 'found'\n",
      "'perfect', 'shopping'\n",
      "'helpful', 'friendly'\n",
      "'helpful', 'recommend', 'planning'\n",
      "'spotless', 'excellent'\n",
      "'clean', 'happy'\n",
      "'friendly', 'helpful'\n",
      "'friendly', 'helpful', 'general'\n",
      "'good'\n",
      "'friendly'\n",
      "'wonderful'\n",
      "'helpful', 'pleasant'\n",
      "'recommend'\n",
      "'clean', 'excellent', 'helpful', 'wonderful'\n",
      "'perfect'\n",
      "'center'\n",
      "'helpful'\n",
      "'excellent', 'provide', 'clean', 'perfect'\n",
      "'oasis'\n",
      "Surprise Words in True Positive Reviews\n",
      "'good'\n",
      "'trip'\n",
      "'trip'\n",
      "'trip'\n",
      "'trip'\n",
      "'wonderfully'\n",
      "'lovely'\n",
      "'wonderful'\n",
      "'money'\n",
      "'pleasant', 'surprise'\n",
      "'wonderful'\n",
      "'good'\n",
      "'shopping'\n",
      "'good'\n",
      "'honeymoon'\n",
      "'shopping'\n",
      "'good'\n",
      "'shopping'\n",
      "'wonderful'\n",
      "'delighted'\n",
      "'smile'\n",
      "'good'\n",
      "'trip', 'shopping'\n",
      "'lovely'\n",
      "'shopping'\n",
      "'good'\n",
      "'pleasant'\n",
      "'money'\n",
      "'shopping'\n",
      "'leave'\n",
      "'good', 'magnificent'\n",
      "'good'\n",
      "'birthday'\n",
      "'trip'\n",
      "'good'\n",
      "'wonderful'\n",
      "'amazingly'\n",
      "'good'\n",
      "'wonderful'\n",
      "'good'\n",
      "'good'\n",
      "'good'\n",
      "'deal'\n",
      "'good'\n",
      "'good'\n",
      "'greeting'\n",
      "'trip', 'birthday'\n",
      "'good'\n",
      "'birthday', 'good'\n",
      "'spectacular'\n",
      "'good', 'entertainment'\n",
      "'wonderful'\n",
      "'pleasant'\n",
      "'good'\n",
      "'good'\n",
      "'good'\n",
      "'wonderful', 'gambling'\n",
      "'pleasant'\n",
      "'good', 'deal'\n",
      "'deal'\n",
      "'good'\n",
      "'unexpectedly', 'pleasant', 'surprise'\n",
      "'trip', 'wonderful'\n",
      "'trip'\n",
      "'wonderful'\n",
      "'good'\n",
      "'good'\n",
      "'good'\n",
      "'magnificent'\n",
      "'trip'\n",
      "'gambling'\n",
      "'magnificent'\n",
      "'deal'\n",
      "'wonderful'\n",
      "'trip'\n",
      "'excited'\n",
      "'magnificent'\n",
      "'good'\n",
      "'cable'\n",
      "'shopping', 'shout', 'gift'\n",
      "'surprised'\n",
      "'blast'\n",
      "'trip'\n",
      "'good'\n",
      "'wonderful'\n",
      "'pleasant'\n",
      "'wonderful'\n",
      "'trip'\n",
      "Joy Words in True Positive Reviews\n",
      "'good'\n",
      "'beautiful'\n",
      "'beautiful'\n",
      "'special'\n",
      "'daughter'\n",
      "'clean', 'excellent', 'helpful'\n",
      "'clean'\n",
      "'clean'\n",
      "'wonderfully'\n",
      "'lovely', 'clean'\n",
      "'friendly', 'wonderful'\n",
      "'pleased'\n",
      "'pleased'\n",
      "'friend'\n",
      "'perfect', 'money'\n",
      "'pleasant', 'surprise'\n",
      "'friendly'\n",
      "'love'\n",
      "'love'\n",
      "'honest'\n",
      "'wonderful'\n",
      "'daughter'\n",
      "'good'\n",
      "'friendly', 'special'\n",
      "'clean'\n",
      "'oasis'\n",
      "'helpful'\n",
      "'perfect'\n",
      "'honest'\n",
      "'beautiful', 'excellent'\n",
      "'love', 'shopping'\n",
      "'outstanding'\n",
      "'good'\n",
      "'renovation'\n",
      "'honeymoon'\n",
      "'shopping', 'food'\n",
      "'clean', 'friendly', 'helpful'\n",
      "'excellent'\n",
      "'favorite'\n",
      "'excellent'\n",
      "'clean'\n",
      "'perfect', 'beautiful', 'friendly'\n",
      "'share'\n",
      "'good'\n",
      "'helpful', 'friendly'\n",
      "'shopping'\n",
      "'perfect', 'friendly'\n",
      "'wonderful'\n",
      "'delighted'\n",
      "'smile'\n",
      "'good'\n",
      "'excellent'\n",
      "'shopping'\n",
      "'lovely', 'excellent'\n",
      "'clean'\n",
      "'perfect'\n",
      "'clean'\n",
      "'shopping'\n",
      "'perfect'\n",
      "'pretty', 'good'\n",
      "'found', 'excellent'\n",
      "'pleasant', 'friendly', 'helpful'\n",
      "'food'\n",
      "'friendly'\n",
      "'excellent', 'money'\n",
      "'friendly'\n",
      "'beautiful', 'clean'\n",
      "'friendly'\n",
      "'shopping'\n",
      "'enjoy'\n",
      "'love'\n",
      "'star'\n",
      "'special'\n",
      "'friendly'\n",
      "'gorgeous'\n",
      "'good', 'magnificent'\n",
      "'helpful', 'good'\n",
      "'friendly', 'birthday'\n",
      "'outstanding'\n",
      "'clean'\n",
      "'excellent'\n",
      "'friendly'\n",
      "'helpful'\n",
      "'good'\n",
      "'wonderful', 'celebrating'\n",
      "'comfort'\n",
      "'amazingly', 'friendly'\n",
      "'pretty', 'good'\n",
      "'wonderful'\n",
      "'beautiful'\n",
      "'gratitude'\n",
      "'beautiful'\n",
      "'good', 'fancy'\n",
      "'grandchildren', 'beautiful', 'food'\n",
      "'good'\n",
      "'friendly'\n",
      "'vacation'\n",
      "'good'\n",
      "'daughter'\n",
      "'love'\n",
      "'love'\n",
      "'found'\n",
      "'fun'\n",
      "'perfect'\n",
      "'clean'\n",
      "'deal'\n",
      "'clean'\n",
      "'helpful'\n",
      "'clean', 'good', 'food', 'found', 'luxurious'\n",
      "'praise'\n",
      "'good'\n",
      "'friendly'\n",
      "'clean'\n",
      "'birthday'\n",
      "'beautiful', 'food'\n",
      "'excellent', 'good'\n",
      "'birthday', 'perfect', 'fun', 'good', 'helpful'\n",
      "'excellent'\n",
      "'good', 'entertainment'\n",
      "'perfect'\n",
      "'wonderful', 'pleased', 'jump', 'gorgeous'\n",
      "'pleasant'\n",
      "'good', 'excellent'\n",
      "'clean'\n",
      "'perfect'\n",
      "'hire'\n",
      "'good'\n",
      "'clean'\n",
      "'found', 'clean'\n",
      "'outstanding'\n",
      "'beautiful'\n",
      "'excellent'\n",
      "'good'\n",
      "'food', 'wonderful', 'fun'\n",
      "'friend'\n",
      "'pleasant'\n",
      "'clean'\n",
      "'outstanding'\n",
      "'good', 'deal'\n",
      "'happy'\n",
      "'clean', 'friendly'\n",
      "'deal', 'helpful'\n",
      "'good'\n",
      "'friendly', 'happy'\n",
      "'perfect', 'clean'\n",
      "'vacation'\n",
      "'friendly'\n",
      "'pleasant', 'surprise'\n",
      "'wonderful'\n",
      "'enjoy'\n",
      "'perfect'\n",
      "'perfect'\n",
      "'perfect', 'clean', 'honest', 'love', 'helpful', 'food'\n",
      "'wonderful', 'helpful'\n",
      "'daughter'\n",
      "'holiday', 'good'\n",
      "'good'\n",
      "'excellent', 'friendly', 'helpful', 'clean'\n",
      "'gem'\n",
      "'good'\n",
      "'friend'\n",
      "'excellent'\n",
      "'magnificent'\n",
      "'green'\n",
      "'clean'\n",
      "'love'\n",
      "'love', 'brilliant'\n",
      "'clean'\n",
      "'magnificent'\n",
      "'quaint'\n",
      "'excellent', 'friendly'\n",
      "'deal', 'friendly'\n",
      "'love', 'wonderful', 'excellent'\n",
      "'friendly'\n",
      "'perfect'\n",
      "'found'\n",
      "'excited', 'diamond'\n",
      "'vacation'\n",
      "'magnificent'\n",
      "'good', 'clean'\n",
      "'marvelous', 'pay', 'found'\n",
      "'perfect', 'shopping', 'special', 'gift'\n",
      "'helpful', 'friendly'\n",
      "'helpful'\n",
      "'gorgeous'\n",
      "'love'\n",
      "'excellent'\n",
      "'clean', 'happy'\n",
      "'friendly', 'helpful'\n",
      "'love'\n",
      "'friendly', 'helpful'\n",
      "'good'\n",
      "'friendly'\n",
      "'wonderful'\n",
      "'helpful', 'pleasant'\n",
      "'beautiful', 'special'\n",
      "'beautiful', 'clean', 'excellent', 'helpful', 'wonderful'\n",
      "'perfect'\n",
      "'helpful'\n",
      "'excellent', 'clean', 'perfect'\n",
      "'luxury', 'oasis'\n"
     ]
    }
   ],
   "source": [
    "#'Anger Words','Ant Words', 'Fear Words', 'Trust Words', 'Surprise Words', 'Joy Words',\n",
    "                #'Disgust Words', 'Sad Words'\n",
    "print('Trust Words in True Positive Reviews')\n",
    "for i in true_positives.index.values:\n",
    "    if true_positives.loc[i]['Trust Words']!='dict_keys([])':\n",
    "        print(true_positives.loc[i]['Trust Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))\n",
    "\n",
    "print('Surprise Words in True Positive Reviews')\n",
    "for i in true_positives.index.values:\n",
    "    if true_positives.loc[i]['Surprise Words']!='dict_keys([])':\n",
    "        print(true_positives.loc[i]['Surprise Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))\n",
    "        \n",
    "print('Joy Words in True Positive Reviews')\n",
    "for i in true_positives.index.values:\n",
    "    if true_positives.loc[i]['Joy Words']!='dict_keys([])':\n",
    "        print(true_positives.loc[i]['Joy Words'].replace(\"dict_keys([\",\"\").replace(\"])\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #: 33891\n",
      "['I booked this hotel because it was cheaper than others, and reviews were ok', 'The registration took nearly an hour (there is a computer for a automatic registration but nobody informed me)', 'There are 6 elevators to the floor but the Hotel was full and the elevarors were not enough, the first time we waited patiently for a elevator (we had valises), fortunately we were in a low floor so, after that, we (as many others) used the stairs', 'The pool is realatively small, it was so full that you bearly could move arround', 'The room was ok (although the icebox was noisy)We got a deal that included breakfast, we thought we would have a calm breakfast in a dining room of a hotel but at the registration, we got informed there is no room, we had to go to one specific coffee shop and spend no more than 9$ (not a good deal because now we had to go to that shop while there are many others nearby)Summary -Not recommend this placeIf I came again I will spend more money and enjoy the hotel', '']\n",
      "\n",
      "\n",
      "Review #: 48961\n",
      "['The two of us found the hostel on a travel agents website expecting a lot more from it from photos we had seen', ' The hostel itself is well priced and looks good on photos so booked up straight away for a week', '', '', \" A little longer than most people stay in Vegas but we were travelling America and couldn't resist 'indulging' in Vegas for a week\", ' From first views it looks okay, friendly and welcoming staff, given towels etc', \" We decided on a private room here so cannot comment on the cleanliness of dorm rooms or shared bathrooms but if it's anything like the kitchen I wouldn't be pleased\", 'The kitchen is a small stainless steel corner of a room with just one stove/oven and one microwave, which is clearly not enough for a hostel full of big dorm rooms, it was hard to find space and/or clean pots or pans to cook with, most of the time we gave up and went to the 7eleven across the street for something quick to eat', ' Pros :- clean pool and spa area to cool off in', '- good sized room and bathroom - helpful friendly staff more than happy to help with anything you need- free pancakes *  Cons : - * offers free pancakes but only for a limited time, also with only one stove and a hostel full of hungry travellers the mixture and space soon gets taken up', ' - shuttle offered was actually just the local bus which cost $8 each one way', \" (Also doesn't drop at hostel) - location couldn't be further from anything\", ' The location of this hostel was the biggest pain for us, being two girls travelling alone, the location of this hostel was most unfriendly', ' On our first night we decided to check out freemont street', ' A good 20 minute walk later you arrive at freemont street experience', ' Though a walk back from there may not seem like long, after the freemont strip ends, it all ends, you are then walking in a derelict once motel ridden area, which lacks any sort of streetlights or sense of security', ' The only residents you see are the local homeless on the street corners and across from the hostel', \" So unsafe so that we were stopped by a man driving past telling us to leave the area and that this wasn't the neighbourhood for us\", \" It was then after ONE night here that we decided we were out of here, we couldn't leave the hostel after dark due to fear of what lurked in the dark corners of an even more downtown downtown Vegas\", \" we booked straight out onto the strip which if I'm honest was not much more money than Las Vegas Hosel\", ' We stayed here a further two nights due to cancellation policy and not wanting to be charged for moving, the next two nights got no better and we felt hostel bound, really not the Vegas experience you expect', ' Overall the hostel has a few minor things that we could have easily worked around and got over but really the area just ruined the whole place for us, leaving us feeling unsafe, trapped in a hostel (that has no locking front door by the way, locals come in as they please)', ' ']\n",
      "\n",
      "\n",
      "Review #: 37393\n",
      "['I have really nightmare stays two nights at this hotel', ' The casino service was Worst', ' especially for the Cocktail waitress near the bar area', ' Very IGNORANT and RUDE!!! and the room was very tiny with tiny bed and so uncomfortable', '']\n",
      "\n",
      "\n",
      "Review #: 24211\n",
      "['It all looks good on from the outside', ' Cool lobby, very trendy restaurant/bar, service oriented staff', '', '', '', ' and then you take the elevator', '', ' The hallways smells old', ' the carpets are dirty - all worn down', ' The rooms are the same', ' Just old and not fresh', ' The bathroom tub was cracking, several tiles had cracks in them too', '', '', ' Simply not what we expected - and prices are high! Everything felt old and dusty', ' Beds were good though', ' If they have renovated rooms, ask for them', '']\n",
      "\n",
      "\n",
      "Review #: 54537\n",
      "['Stayed here for two nights', ' This was my second time at the Park Central (I would have preferred to stay elsewhere but was with a group of people staying here so it was just easier)', ' The hotel is fine, the rooms are nice and the location is great (and easy to access on the BART from SFO)', ' My only major gripe (and others on here have pointed this out) is the $44', '60 \"resort fee,\" which was about $30 on my last stay', ' Yes, it is made clear when you are booking the room (though I guess some very casual travelers might overlook it)', ' But I still do not get the point', ' First, in what sense is the Park Central a \"resort\"? Second, and more importantly, if the flat fee is added to every room for every night, why not just include it in the room rate? Maybe the future will bring a $40', '00 room rate with a $120', '00 resort fee?']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in false_positives.index.values[:5]:\n",
    "    print('Review #: ' + str(i))\n",
    "    print(false_positives.loc[i]['review_body'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #: 13772\n",
      "['We used to live not far from the Marcel and I am in its neighborhood almost weekly, but had never noticed the hotel before my husband made a reservation there for a birthday weekend', \" I was actually a little apprehensive when he told me where he had reserved the room, because I couldn't imagine what the hotel would be like\", ' There was no need to worry', ' The experience from check in to check out was great', ' The front desk staff was friendly and efficient', ' The room was spotless and surprisingly spacious for Manhattan', ' The modern style of both the room and the lobby was appealing', ' There was even complementary coffee in an upstairs guest lounge and a continental breakfast available in the morning for a separate fee', \" Although we opted to eat at a favorite breakfast destination in the neighborhood (Bluebell CafÃ©, on Third Avenue and 22nd Street), the hotel's option looked good\", \" We will definitely return in the future, as the location is convenient to some of the city's best restaurants\", ' A great find, thanks to Trip Advisor!']\n",
      "\n",
      "\n",
      "Review #: 16396\n",
      "['I am a Las Vegas local', ' So i have a number of gaming choices in the valley', ' But when i go to Ceasers Palace the staff makes me feel at home', \" Clean facility and friendly staff, can't ask for better than this!\"]\n",
      "\n",
      "\n",
      "Review #: 13442\n",
      "['We chose this hotel because we loveGrammercy and its location at 24 th and 3rd ave', ' We walked everywhere from there', 'The front desk was wonderful in providing extra pillows and ice', ' we would definitely stay there again']\n",
      "\n",
      "\n",
      "Review #: 43580\n",
      "['Our room (on the 19th) floor was very quiet', ' Had a great view of the High Roller ferris wheel (which did cost extra) - was very pretty at night with all the colors', ' Rooms were very clean, concierge was extremely helpful and friendly', ' The restaurants at the Linq were very good', ' Checking in and out was very quick, only had one slightly unfriendly woman in that area, wish I had gotten her name; otherwise no complaints in that area', ' Sometimes had a long wait for the elevator, but that was about it', ' Really enjoyed the Linq and would highly recommend it!']\n",
      "\n",
      "\n",
      "Review #: 34915\n",
      "['I wanted to stay here last year but my trip was cancelled, so I was excited to finally stay this trip', ' Check in was a breeze between the kiosks and the desk', ' It probably took me 10 minutes to check in', \" The room was really cute and I wasn't concerned about a view as we were only staying one night\", ' The beds were super comfortable, the bathroom was a great size, not too far from the elevators (I think we were in District 2)', \" The Linq's locations is awesome, right in the middle so not too far from North or South Strip\", \" The Promenade has lots of great restaurants and hangout spots just outside of the hotel, so it's very convenient\", ' We enjoyed our short stay and will definitely be back!']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in false_negatives.index.values[:5]:\n",
    "    print('Review #: ' + str(i))\n",
    "    print(false_negatives.loc[i]['review_body'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Value Comparisons btw True/False + -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False+ | Polarity2: 0.3185039569805195 True |-0.10390411850463396\n",
      "False+ | Subjectivity: 0.4803929642406205 True |0.3857907127237024\n",
      "False+ | Word_Count: 14 True |20\n",
      "False+ | Helpful: 20 True |11\n",
      "False+ | affect: 0.0546375 True |0.045620103092783505\n",
      "False+ | cogproc: 0.1025796875 True |0.09956494845360825\n",
      "False+ | Trauma: 0 True |0\n",
      "False+ | spatial: 1 True |1\n",
      "False+ | temporal: 1 True |1\n",
      "False+ | Anger: 0.003037052266081871 True |0.017211995633945106\n",
      "False+ | Anticipation: 0.02483077202191785 True |0.019008265004702207\n",
      "False+ | Trust: 0.03538472189632535 True |0.01637514573586462\n",
      "False+ | Surprise: 0.016319369792695257 True |0.006145783411439398\n",
      "False+ | Sad: 0.002441454586701864 True |0.015270775933676506\n",
      "False+ | Joy: 0.029703574537062665 True |0.013831640934790813\n",
      "False+ | Disgust: 0.0012335526315789473 True |0.019013842023626937\n",
      "False+ | Fear: 0.005343599885129491 True |0.012988083678996414\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "print('False+ | Polarity2: ' + str(mean(false_positives.Polarity2)) + ' True |' +str(mean(true_negatives.Polarity2)))\n",
    "print('False+ | Subjectivity: ' + str(mean(false_positives.Subjectivity))+ ' True |' +str(mean(true_negatives.Subjectivity)))\n",
    "print('False+ | Word_Count: ' + str(mean(false_positives.Word_Count))+ ' True |' +str(mean(true_negatives.Word_Count)))\n",
    "print('False+ | Helpful: ' + str(mean(false_positives.Helpful))+ ' True |' +str(mean(true_negatives.Helpful)))\n",
    "print('False+ | affect: ' + str(mean(false_positives.affect))+ ' True |' +str(mean(true_negatives.affect)))\n",
    "print('False+ | cogproc: ' + str(mean(false_positives.cogproc))+ ' True |' +str(mean(true_negatives.cogproc)))\n",
    "print('False+ | Trauma: ' + str(mean(false_positives.Trauma))+ ' True |' +str(mean(true_negatives.Trauma)))\n",
    "print('False+ | spatial: ' + str(mean(false_positives.spatial))+ ' True |' +str(mean(true_negatives.spatial)))\n",
    "print('False+ | temporal: ' + str(mean(false_positives.temporal))+ ' True |' +str(mean(true_negatives.temporal)))\n",
    "print('False+ | Anger: ' + str(mean(false_positives.Anger))+ ' True |' +str(mean(true_negatives.Anger)))\n",
    "print('False+ | Anticipation: ' + str(mean(false_positives.Anticipation))+ ' True |' +str(mean(true_negatives.Anticipation)))\n",
    "print('False+ | Trust: ' + str(mean(false_positives.Trust))+ ' True |' +str(mean(true_negatives.Trust)))\n",
    "print('False+ | Surprise: ' + str(mean(false_positives.Surprise))+ ' True |' +str(mean(true_negatives.Surprise)))\n",
    "print('False+ | Sad: ' + str(mean(false_positives.Sad))+ ' True |' +str(mean(true_negatives.Sad)))\n",
    "print('False+ | Joy: ' + str(mean(false_positives.Joy))+ ' True |' +str(mean(true_negatives.Joy)))\n",
    "print('False+ | Disgust: ' + str(mean(false_positives.Disgust))+ ' True |' +str(mean(true_negatives.Disgust)))\n",
    "print('False+ | Fear: ' + str(mean(false_positives.Fear))+ ' True |' +str(mean(true_negatives.Fear)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False- | Polarity2: 0.030926157739266505 True |0.45260317134573774\n",
      "False- | Subjectivity: 0.2445374188752165 True |0.6102787830994125\n",
      "False- | Word_Count: 18 True |16\n",
      "False- | Helpful: 10 True |18\n",
      "False- | affect: 0.05787575757575757 True |0.09004953703703704\n",
      "False- | cogproc: 0.09607121212121213 True |0.07933402777777777\n",
      "False- | Trauma: 0 True |0\n",
      "False- | spatial: 1 True |1\n",
      "False- | temporal: 1 True |1\n",
      "False- | Anger: 0.004755669105717281 True |0.0022862478577837363\n",
      "False- | Anticipation: 0.021757516973061646 True |0.02855271164296416\n",
      "False- | Trust: 0.015951096406668357 True |0.04114897861434589\n",
      "False- | Surprise: 0.012525183356726612 True |0.01451659588388738\n",
      "False- | Sad: 0.008381353693547713 True |0.004980185046805211\n",
      "False- | Joy: 0.013792447953588947 True |0.045059468014049135\n",
      "False- | Disgust: 0.003536604240829593 True |0.0013586385645043368\n",
      "False- | Fear: 0.008246246133899968 True |0.0016556714100557355\n"
     ]
    }
   ],
   "source": [
    "print('False- | Polarity2: ' + str(mean(false_negatives.Polarity2))+ ' True |' +str(mean(true_positives.Polarity2)))\n",
    "print('False- | Subjectivity: ' + str(mean(false_negatives.Subjectivity))+ ' True |' +str(mean(true_positives.Subjectivity)))\n",
    "print('False- | Word_Count: ' + str(mean(false_negatives.Word_Count))+ ' True |' +str(mean(true_positives.Word_Count)))\n",
    "print('False- | Helpful: ' + str(mean(false_negatives.Helpful))+ ' True |' +str(mean(true_positives.Helpful)))\n",
    "print('False- | affect: ' + str(mean(false_negatives.affect))+ ' True |' +str(mean(true_positives.affect)))\n",
    "print('False- | cogproc: ' + str(mean(false_negatives.cogproc))+ ' True |' +str(mean(true_positives.cogproc)))\n",
    "print('False- | Trauma: ' + str(mean(false_negatives.Trauma))+ ' True |' +str(mean(true_positives.Trauma)))\n",
    "print('False- | spatial: ' + str(mean(false_negatives.spatial))+ ' True |' +str(mean(true_positives.spatial)))\n",
    "print('False- | temporal: ' + str(mean(false_negatives.temporal))+ ' True |' +str(mean(true_positives.temporal)))\n",
    "print('False- | Anger: ' + str(mean(false_negatives.Anger))+ ' True |' +str(mean(true_positives.Anger)))\n",
    "print('False- | Anticipation: ' + str(mean(false_negatives.Anticipation))+ ' True |' +str(mean(true_positives.Anticipation)))\n",
    "print('False- | Trust: ' + str(mean(false_negatives.Trust))+ ' True |' +str(mean(true_positives.Trust)))\n",
    "print('False- | Surprise: ' + str(mean(false_negatives.Surprise))+ ' True |' +str(mean(true_positives.Surprise)))\n",
    "print('False- | Sad: ' + str(mean(false_negatives.Sad))+ ' True |' +str(mean(true_positives.Sad)))\n",
    "print('False- | Joy: ' + str(mean(false_negatives.Joy))+ ' True |' +str(mean(true_positives.Joy)))\n",
    "print('False- | Disgust: ' + str(mean(false_negatives.Disgust))+ ' True |' +str(mean(true_positives.Disgust)))\n",
    "print('False- | Fear: ' + str(mean(false_negatives.Fear))+ ' True |' +str(mean(true_positives.Fear)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
