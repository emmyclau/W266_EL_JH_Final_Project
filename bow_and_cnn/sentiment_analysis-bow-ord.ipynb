{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the BOW model with Ordinal Output Categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt \n",
    "import glob\n",
    "from importlib import reload\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from common import utils, constants, spell\n",
    "\n",
    "import tensorflow as tf\n",
    "import tripadvisor_ds\n",
    "import visualization\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load tripadvisor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(tripadvisor_ds)\n",
    "\n",
    "input_length = 500\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "data_file = 'data/tripadvisor_ds.pkl'\n",
    "\n",
    "if os.path.isfile(data_file):\n",
    "\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(data_file)\n",
    "    with open(data_file, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    ds = pickle.loads(bytes_in)\n",
    "        \n",
    "else:\n",
    "    ds = tripadvisor_ds.TripAdvisor_DS().process(input_length=input_length)\n",
    "    ds.save(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# convert the output to Ordinal Categories \n",
    "# 2-star rating: [0,0,0]\n",
    "# 3-star rating: [1,0,0]\n",
    "# 4-star rating: [1,1,0]\n",
    "# 5-star rating: [1,1,1]\n",
    "# -----------------------------------------\n",
    "\n",
    "ds.get_ord_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    413463\n",
       "2    231827\n",
       "1    106079\n",
       "0     90053\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame({'rating': ds.train_labels})\n",
    "labels.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set model parameters \n",
    "model_params = dict(V=ds.vocab.size, \n",
    "                    embed_dim=100, \n",
    "                    num_classes=len(ds.target_labels),\n",
    "                    encoder_type='bow', \n",
    "                    hidden_dims=[1024, 64], \n",
    "                    input_length=input_length,\n",
    "                    lr=0.0001, \n",
    "                    optimizer='adam', \n",
    "                    beta=0.00001)\n",
    "                    \n",
    "train_params = dict(batch_size=64, \n",
    "                    total_epochs=20, \n",
    "                    eval_every=2)\n",
    "\n",
    "\n",
    "summary_params = dict(chkpt_dir=\"./tmp/266_bow_ord\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup tensorboard \n",
    "\n",
    "if os.path.isdir(summary_params['chkpt_dir']):\n",
    "    shutil.rmtree(summary_params['chkpt_dir'])\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    # prepare vocabulary  \n",
    "    W_embed_ = tf.get_variable(\"W_embed\", shape=[V, embed_dim], \\\n",
    "                               initializer=tf.random_uniform_initializer(-init_scale, init_scale), \\\n",
    "                               trainable=True)\n",
    "        \n",
    "    # look up word embedding \n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_, name=\"embed_x\")\n",
    "        \n",
    "    return xs_\n",
    "\n",
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.nn.relu,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training )\n",
    "\n",
    "    return h_\n",
    "\n",
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    W_out_ = tf.get_variable(\"W_out\",  shape=[h_.get_shape().as_list()[1], num_classes], \\\n",
    "                               initializer=tf.random_normal_initializer())\n",
    "    b_out_ = tf.get_variable(\"b_out\", shape=[num_classes])\n",
    "\n",
    "    logits_ = tf.add(tf.matmul(h_, W_out_), b_out_)\n",
    "        \n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "    \n",
    "    with tf.variable_scope(\"Softmax_Layer\"):\n",
    "\n",
    "        softmax_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_, logits=logits_)\n",
    "        \n",
    "        loss_ = tf.reduce_mean(softmax_)\n",
    "    \n",
    "    return loss_, logits_\n",
    "\n",
    "\n",
    "def sigmoid_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    W_out_ = tf.get_variable(\"W_out\",  shape=[h_.get_shape().as_list()[1], num_classes], \\\n",
    "                               initializer=tf.random_normal_initializer())\n",
    "    b_out_ = tf.get_variable(\"b_out\", shape=[num_classes])\n",
    "\n",
    "    logits_ = tf.add(tf.matmul(h_, W_out_), b_out_)\n",
    "        \n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "    \n",
    "    with tf.variable_scope(\"Sigmoid_Layer\"):\n",
    "        \n",
    "        sigmoid_ = tf.nn.sigmoid(logits_)\n",
    "        \n",
    "        loss_ = num_classes * tf.reduce_mean(tf.squared_difference( labels_, sigmoid_))\n",
    "        \n",
    "    return loss_, logits_\n",
    "\n",
    "\n",
    "def BOW(ids_, V, embed_dim, hidden_dims, dropout_rate=0, is_training=None):\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "     \n",
    "    sum_xs_ = tf.reduce_sum(xs_, 1)\n",
    "\n",
    "    h_ = fully_connected_layers(sum_xs_, hidden_dims, \\\n",
    "                           dropout_rate=dropout_rate, is_training=is_training)\n",
    "    return h_, xs_\n",
    "\n",
    "\n",
    "def conv_net(ids_, V, embed_dim, filter_sizes, num_filters, hidden_dims, input_length, dropout_rate=0, is_training=None):\n",
    "\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "\n",
    "    xs_ = tf.expand_dims(xs_, -1)\n",
    "        \n",
    "    pooled_outputs_ = []\n",
    "    for _, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"Conv_MaxPool_%d\"%filter_size):\n",
    "            \n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, embed_dim, 1, num_filters]\n",
    "            W_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b_ = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv_ = tf.nn.conv2d(\n",
    "                xs_,\n",
    "                W_,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Activation\n",
    "            h_ = tf.nn.relu(tf.nn.bias_add(conv_, b_), name=\"relu\")\n",
    "            \n",
    "            # Maxpooling \n",
    "            pooled_ = tf.nn.max_pool(\n",
    "                h_,\n",
    "                ksize=[1, input_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs_.append(pooled_)\n",
    "            \n",
    "            variable_summaries(pooled_)\n",
    "\n",
    "    # Combine all the pooled features and flatten it\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_ = tf.concat(pooled_outputs_, 3)\n",
    "    h_ = tf.reshape(h_, [-1, num_filters_total])\n",
    "    \n",
    "    # fully connected layers\n",
    "    with tf.variable_scope(\"FC_Layer\"):\n",
    "        h_ = fully_connected_layers(h_, hidden_dims, is_training = is_training)\n",
    "\n",
    "    return h_, xs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, input_length], name='input_x')\n",
    "Y = tf.placeholder(tf.float32, [None, 3], name='input_y')\n",
    "    \n",
    "if model_params['encoder_type'] == 'bow':\n",
    "    h_, xs_ = BOW(X, model_params['V'], \n",
    "                      model_params['embed_dim'],  \n",
    "                      model_params['hidden_dims'],\n",
    "                      is_training=True)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    loss_, logits_ = sigmoid_output_layer(h_, Y, model_params['num_classes'] -1)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.sigmoid(logits_, name=\"pred_proba\")\n",
    "    \n",
    "    pred_max_ = tf.reduce_sum(tf.cast(tf.great(pred_proba_ ,0.5), tf.int32), axis = 1, name=\"pred_max\")\n",
    "    \n",
    "    predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "with tf.variable_scope(\"Regularization\"):\n",
    "    l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "    for var_ in tf.trainable_variables():\n",
    "        if \"Embedding_Layer\" in var_.name:\n",
    "            continue\n",
    "        l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "    l2_penalty_ *= model_params['beta']  # scale by regularization strength\n",
    "    tf.summary.scalar('l2_penalty', l2_penalty_)\n",
    "    regularized_loss_ = loss_ + l2_penalty_\n",
    "    tf.summary.scalar('regularized_loss', regularized_loss_)\n",
    "\n",
    "with tf.variable_scope(\"Training\"):\n",
    "    if model_params['optimizer'] == 'adagrad':\n",
    "        optimizer_ = tf.train.AdagradOptimizer(model_params['lr'])\n",
    "    elif  model_params['optimizer'] == 'adam':\n",
    "        optimizer_ = tf.train.AdamOptimizer(model_params['lr'])\n",
    "    else:\n",
    "        optimizer_ = tf.train.GradientDescentOptimizer(model_params['lr'])\n",
    "    train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                    global_step=tf.train.get_global_step())\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "\n",
    "    correct_pred_ = tf.equal(tf.cast(pred_max_, tf.int32), tf.cast(tf.reduce_sum(Y, 1), tf.int32))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_pred_, tf.float32))\n",
    "\n",
    "    tf.summary.scalar('loss', loss_)\n",
    "    tf.summary.scalar('accuracy', accuracy_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# Tensorboard - Visualize graph \n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/test')\n",
    "\n",
    "print(\"tensorboard --logdir={}/train\".format(summary_params['chkpt_dir']))\n",
    "print(\"tensorboard --logdir={}/test\".format(summary_params['chkpt_dir']))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "sess.run(init_l)\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(train_params['total_epochs']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_batches = 1\n",
    "    train_accuracy = 0.0\n",
    "    \n",
    "    for (bx, by) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_train_features, ds.train_ord_labels):\n",
    "\n",
    "        summary, batch_loss, _, batch_accuracy, pred_proba, pred_max = sess.run(\n",
    "            [merged, regularized_loss_, train_op_, accuracy_, pred_proba_, pred_max_], feed_dict={X: bx, Y: by})\n",
    "        \n",
    "        #print(pred_proba)\n",
    "        #print(pred_max)\n",
    "        \n",
    "        train_batches +=1\n",
    "        train_accuracy += batch_accuracy\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}, train accuracy {:.2f}\"\\\n",
    "                  .format(total_examples, loss_ema, train_accuracy/train_batches))    \n",
    "            \n",
    "        train_writer.add_summary(summary, total_batches)\n",
    "\n",
    "    print(\"Completed {} epoch in {:s}\".format(i, utils.pretty_timedelta(since=t0)))\n",
    "    \n",
    "    train_accuracy = train_accuracy/train_batches\n",
    "    print(\"Train accurary:{:.5f}\".format(train_accuracy))\n",
    "    \n",
    "    \n",
    "    # run the validation dataset \n",
    "    validate_batches = 1\n",
    "    validate_accuracy = 0.0\n",
    "    for (vx, vy) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                            ds.padded_validate_features, ds.validate_ord_labels):\n",
    "\n",
    "        summary, batch_accuracy = sess.run([merged, accuracy_], feed_dict={X: vx, Y: vy})\n",
    "\n",
    "        validate_batches +=1\n",
    "        validate_accuracy += batch_accuracy\n",
    "\n",
    "        test_writer.add_summary(summary, total_batches + validate_batches)\n",
    "\n",
    "    validate_accuracy = validate_accuracy/validate_batches\n",
    "    print(\"Validate accuracy:{:.5f}\".format(validate_accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.7119574175981375\n"
     ]
    }
   ],
   "source": [
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_test_features, ds.test_ord_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "\n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Test accuracy:{}\".format(test_accuracy))\n",
    "\n",
    "pred_y = [y for x in test_pred_y for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7718,  2946,   257,    35],\n",
       "       [ 1593,  7844,  3351,   243],\n",
       "       [  224,  3233, 16614,  8606],\n",
       "       [   65,   561,  8754, 41835]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ds.test_labels, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#op = sess.graph.get_operations()\n",
    "#[m.values() for m in op]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
