{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the BOW Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt \n",
    "import glob\n",
    "from importlib import reload\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from common import utils, constants, spell\n",
    "\n",
    "import tensorflow as tf\n",
    "import tripadvisor_ds\n",
    "import visualization\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load tripadvisor dataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(tripadvisor_ds)\n",
    "\n",
    "input_length = 500\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "data_file = 'data/tripadvisor_ds.pkl'\n",
    "\n",
    "if os.path.isfile(data_file):\n",
    "\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(data_file)\n",
    "    with open(data_file, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    ds = pickle.loads(bytes_in)\n",
    "        \n",
    "else:\n",
    "    ds = tripadvisor_ds.TripAdvisor_DS().process(input_length=input_length)\n",
    "    ds.save(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    413463\n",
       "2    231827\n",
       "1    106079\n",
       "0     90053\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame({'rating': ds.train_labels})\n",
    "labels.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set model parameters \n",
    "model_params = dict(V=ds.vocab.size, \n",
    "                    embed_dim=100, \n",
    "                    num_classes=len(ds.target_labels),\n",
    "                    encoder_type='bow', \n",
    "                    hidden_dims=[1024, 64], \n",
    "                    input_length=input_length,\n",
    "                    lr=0.0001, \n",
    "                    optimizer='adam', \n",
    "                    beta=0.0001)\n",
    "                    \n",
    "train_params = dict(batch_size=64, \n",
    "                    total_epochs=20, \n",
    "                    eval_every=2)\n",
    "\n",
    "\n",
    "summary_params = dict(chkpt_dir=\"./tmp/266_cnn_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup tensorboard \n",
    "\n",
    "if os.path.isdir(summary_params['chkpt_dir']):\n",
    "    shutil.rmtree(summary_params['chkpt_dir'])\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(\"W_embed\", shape=[V, embed_dim], \\\n",
    "                               initializer=tf.random_uniform_initializer(-init_scale, init_scale), \\\n",
    "                               trainable=True)\n",
    "    \n",
    "    #W_embed_ = tf.constant(embedding_matrix, dtype=tf.float32, name=\"W_embed\")\n",
    "    \n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_, name=\"embed_x\")\n",
    "        \n",
    "    return xs_\n",
    "\n",
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.nn.relu,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "        if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training )\n",
    "            h_ = tf.layers.dropout(h_, training=is_training )\n",
    "    return h_\n",
    "\n",
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    W_out_ = tf.get_variable(\"W_out\",  shape=[h_.get_shape().as_list()[1], num_classes], \\\n",
    "                               initializer=tf.random_normal_initializer())\n",
    "    b_out_ = tf.get_variable(\"b_out\", shape=[num_classes])\n",
    "\n",
    "    logits_ = tf.add(tf.matmul(h_, W_out_), b_out_)\n",
    "        \n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "    \n",
    "    with tf.variable_scope(\"Softmax_Layer\"):\n",
    "\n",
    "        softmax_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_, logits=logits_)\n",
    "        \n",
    "        loss_ = tf.reduce_mean(softmax_)\n",
    "    \n",
    "    return loss_, logits_\n",
    "\n",
    "def BOW(ids_, V, embed_dim, hidden_dims, dropout_rate=0, is_training=None):\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "     \n",
    "    sum_xs_ = tf.reduce_sum(xs_, 1)\n",
    "\n",
    "    h_ = fully_connected_layers(sum_xs_, hidden_dims, \\\n",
    "                           dropout_rate=dropout_rate, is_training=is_training)\n",
    "    return h_, xs_\n",
    "\n",
    "\n",
    "def conv_net(ids_, V, embed_dim, filter_sizes, num_filters, hidden_dims, input_length, dropout_rate=0, is_training=None):\n",
    "\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "\n",
    "    xs_ = tf.expand_dims(xs_, -1)\n",
    "        \n",
    "    pooled_outputs_ = []\n",
    "    for _, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"Conv_MaxPool_%d\"%filter_size):\n",
    "            \n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, embed_dim, 1, num_filters]\n",
    "            W_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b_ = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv_ = tf.nn.conv2d(\n",
    "                xs_,\n",
    "                W_,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Activation\n",
    "            h_ = tf.nn.relu(tf.nn.bias_add(conv_, b_), name=\"relu\")\n",
    "            \n",
    "            # Maxpooling \n",
    "            pooled_ = tf.nn.max_pool(\n",
    "                h_,\n",
    "                ksize=[1, input_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs_.append(pooled_)\n",
    "            \n",
    "            variable_summaries(pooled_)\n",
    "\n",
    "    # Combine all the pooled features and flatten it\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_ = tf.concat(pooled_outputs_, 3)\n",
    "    h_ = tf.reshape(h_, [-1, num_filters_total])\n",
    "    \n",
    "    # fully connected layers\n",
    "    with tf.variable_scope(\"FC_Layer\"):\n",
    "        h_ = fully_connected_layers(h_, hidden_dims, is_training = is_training)\n",
    "\n",
    "    return h_, xs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, input_length], name='input_x')\n",
    "Y = tf.placeholder(tf.int32, [None,], name='input_y')\n",
    "    \n",
    "\n",
    "if Y is None:\n",
    "    one_hot_y_ = None\n",
    "else:\n",
    "    one_hot_y_ = tf.one_hot(Y, model_params['num_classes'])\n",
    "\n",
    "\n",
    "if model_params['encoder_type'] == 'cnn':\n",
    "    h_, xs_ = conv_net(X, model_params['V'], \n",
    "                      model_params['embed_dim'], \n",
    "                      model_params['filter_sizes'], \n",
    "                      model_params['num_filters'], \n",
    "                      model_params['hidden_dims'],\n",
    "                      model_params['input_length'],\n",
    "                      is_training=True)\n",
    "elif model_params['encoder_type'] == 'bow':\n",
    "    h_, xs_ = BOW(X, model_params['V'], \n",
    "                      model_params['embed_dim'],  \n",
    "                      model_params['hidden_dims'],\n",
    "                      is_training=True)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    ce_loss_, logits_ = softmax_output_layer(h_, Y, model_params['num_classes'])\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "with tf.variable_scope(\"Regularization\"):\n",
    "    l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "    for var_ in tf.trainable_variables():\n",
    "        if \"Embedding_Layer\" in var_.name:\n",
    "            continue\n",
    "        l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "    l2_penalty_ *= model_params['beta']  # scale by regularization strength\n",
    "    tf.summary.scalar('l2_penalty', l2_penalty_)\n",
    "    regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "    tf.summary.scalar('regularized_loss', regularized_loss_)\n",
    "\n",
    "with tf.variable_scope(\"Training\"):\n",
    "    if model_params['optimizer'] == 'adagrad':\n",
    "        optimizer_ = tf.train.AdagradOptimizer(model_params['lr'])\n",
    "    elif  model_params['optimizer'] == 'adam':\n",
    "        optimizer_ = tf.train.AdamOptimizer(model_params['lr'])\n",
    "    else:\n",
    "        optimizer_ = tf.train.GradientDescentOptimizer(model_params['lr'])\n",
    "    train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                    global_step=tf.train.get_global_step())\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    cross_entropy_loss_ = tf.metrics.mean(ce_loss_)\n",
    "    \n",
    "    correct_pred_ = tf.equal(tf.argmax(logits_, 1), tf.argmax(one_hot_y_, 1))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_pred_, tf.float32))\n",
    "\n",
    "    eval_metrics = {\"cross_entropy_loss\": cross_entropy_loss_, \"accuracy\": accuracy_}\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy', ce_loss_)\n",
    "    tf.summary.scalar('accuracy', accuracy_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# Tensorboard - Visualize graph \n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/test')\n",
    "\n",
    "print(\"tensorboard --logdir={}/train\".format(summary_params['chkpt_dir']))\n",
    "print(\"tensorboard --logdir={}/test\".format(summary_params['chkpt_dir']))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "sess.run(init_l)\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(train_params['total_epochs']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_batches = 1\n",
    "    train_accuracy = 0.0\n",
    "    \n",
    "    for (bx, by) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_train_features, ds.train_labels):\n",
    "\n",
    "        summary, batch_loss, _, batch_accuracy = sess.run(\n",
    "            [merged, regularized_loss_, train_op_, accuracy_], feed_dict={X: bx, Y: by})\n",
    "        \n",
    "        train_batches +=1\n",
    "        train_accuracy += batch_accuracy\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}, train accuracy {:.2f}\"\\\n",
    "                  .format(total_examples, loss_ema, train_accuracy/train_batches))    \n",
    "            \n",
    "        train_writer.add_summary(summary, total_batches)\n",
    "\n",
    "    print(\"Completed {} epoch in {:s}\".format(i, utils.pretty_timedelta(since=t0)))\n",
    "    \n",
    "    train_accuracy = train_accuracy/train_batches\n",
    "    print(\"Train accurary:{:.5f}\".format(train_accuracy))\n",
    "    \n",
    "    \n",
    "    # run the validation dataset \n",
    "    validate_batches = 1\n",
    "    validate_accuracy = 0.0\n",
    "    for (vx, vy) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                            ds.padded_validate_features, ds.validate_labels):\n",
    "\n",
    "        summary, batch_accuracy = sess.run([merged, accuracy_], feed_dict={X: vx, Y: vy})\n",
    "\n",
    "        validate_batches +=1\n",
    "        validate_accuracy += batch_accuracy\n",
    "\n",
    "        test_writer.add_summary(summary, total_batches + validate_batches)\n",
    "\n",
    "    validate_accuracy = validate_accuracy/validate_batches\n",
    "    print(\"Validate accuracy:{:.5f}\".format(validate_accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.7110148514851485\n"
     ]
    }
   ],
   "source": [
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_test_features, ds.test_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Test accuracy:{}\".format(test_accuracy))\n",
    "\n",
    "pred_y = [y for x in test_pred_y for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8244,  2245,   367,   100],\n",
       "       [ 1902,  6760,  3952,   417],\n",
       "       [  256,  2283, 16288,  9850],\n",
       "       [   69,   261,  7482, 43403]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ds.test_labels, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#op = sess.graph.get_operations()\n",
    "#[m.values() for m in op]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Integrated Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_gradients(sess, graph, review):\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        # get prediction \n",
    "        pred_prob_ = graph.get_tensor_by_name('Prediction/pred_proba:0')\n",
    "        pred_prob = sess.run([pred_prob_], feed_dict={X:[review], Y:[0]})\n",
    "        \n",
    "        # get gradient     \n",
    "        input_y_ = graph.get_tensor_by_name('input_y:0')[0]\n",
    "        embed_x_ = graph.get_tensor_by_name('Embedding_Layer/embed_x:0')\n",
    "        label_prediction_ = graph.get_tensor_by_name('Prediction/pred_proba:0')[:, input_y_]\n",
    "        grads_ = tf.gradients(label_prediction_, embed_x_)[0]\n",
    "        \n",
    "        embed_x, grads = sess.run([embed_x_, grads_], feed_dict={X:[review], Y:[np.argmax(pred_prob)]})\n",
    "        return grads * embed_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrated_gradients_new(sess, graph, review, steps = 50):\n",
    "\n",
    "    empty_review = np.zeros(input_length)\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        # get embedding and prediction \n",
    "        embed_x_ = graph.get_tensor_by_name('Embedding_Layer/embed_x:0')\n",
    "        pred_prob_ = graph.get_tensor_by_name('Prediction/pred_proba:0')\n",
    "        embed_x, pred_y, pred_prob = sess.run([embed_x_, pred_max_, pred_prob_], feed_dict={X:[review], Y:[4]})\n",
    "          \n",
    "        # empty embedding \n",
    "        empty_embed_x, empty_pred_prob = sess.run([embed_x_, pred_prob_], feed_dict={X:[empty_review], Y:[4]})\n",
    "            \n",
    "            \n",
    "        # get integrated gradient \n",
    "        input_y_ = graph.get_tensor_by_name('input_y:0')[0]\n",
    "        label_prediction_ = graph.get_tensor_by_name('Prediction/pred_proba:0')[:, input_y_]\n",
    "        grads_ = tf.gradients(label_prediction_, embed_x_)[0]\n",
    "\n",
    "        \n",
    "        all_grads = []\n",
    "        for i in range(0, steps + 1):\n",
    "            _, grads = sess.run([label_prediction_, grads_], feed_dict={embed_x_:(empty_embed_x + (embed_x - empty_embed_x)*(float(i)/steps)), Y:pred_y})\n",
    "            all_grads.append(grads)\n",
    "        \n",
    "        integrated_grads = np.average(all_grads[:-1], axis=0) * embed_x\n",
    "        return integrated_grads, pred_y[0], pred_prob, empty_pred_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 supporing classes to save the output of the Integrated Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttributionResult(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.html_text = \"<html><body>\"\n",
    "    \n",
    "    def append(self, text):\n",
    "        self.html_text += text\n",
    "    \n",
    "    def write(self, index):\n",
    "        self.html_text += \"</body></html>\"\n",
    "        \n",
    "        with open(\"result/{}_{}.html\".format(self.filename, index), \"w\") as the_file:\n",
    "            the_file.write(self.html_text)\n",
    "            \n",
    "        self.html_text = \"<html><body>\"\n",
    "        \n",
    "class AttributionCount(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.word_count = []\n",
    "        \n",
    "    def append(self, token):\n",
    "        self.word_count.append(token)\n",
    "    \n",
    "    def write(self, index):\n",
    "        df = pd.DataFrame.from_records(self.word_count)\n",
    "        df.columns = ['pred', 'target', 'vocab', 'attr']\n",
    "        df.attr.round(5)\n",
    "        df.to_csv(\"result/{}_{}.csv\".format(self.filename, index))\n",
    "    \n",
    "        self.word_count = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize attributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(visualization)\n",
    "\n",
    "# save the attributions \n",
    "file_suffix = \"bow_d{}_h1024_64_epoch{}\".format(model_params['embed_dim'], \\\n",
    "                                   train_params['total_epochs'])\n",
    "\n",
    "corrHigh = AttributionResult(\"CorrHigh_{}\".format(file_suffix))\n",
    "corrLow = AttributionResult(\"CorrLow_{}\".format(file_suffix))\n",
    "incorrHigh = AttributionResult(\"InCorrHigh_{}\".format(file_suffix))\n",
    "incorrLow = AttributionResult(\"InCorrLow_{}\".format(file_suffix))\n",
    "word_counts = AttributionCount(\"wordcounts_{}\".format(file_suffix))\n",
    "\n",
    "\n",
    "# loop through the test data to visualize the result \n",
    "\n",
    "for i in range(0, len(ds.test_labels)):\n",
    "    gradients = get_gradients(sess, sess.graph, ds.padded_test_features[i])\n",
    "    integrated_gradients, pred_y, pred_proba, empty_pred_proba = \\\n",
    "                                get_integrated_gradients_new(sess, sess.graph, ds.padded_test_features[i])\n",
    "   \n",
    "    \n",
    "    #print(np.matrix(gradients).sum())\n",
    "    print(np.matrix(integrated_gradients).sum())\n",
    "    print(pred_proba[0][pred_y])\n",
    "    print(empty_pred_proba[0][pred_y])\n",
    "\n",
    "    pred_text = \"predicted = {}, actual = {}\".format(pred_y+2, ds.test_labels[i]+2)\n",
    "    print(\"{}, {}\".format(i, pred_text))\n",
    "\n",
    "    html_text = visualization.visualize_token_attrs(\n",
    "            ds.vocab.ids_to_words(ds.test_features[i])[:input_length], \n",
    "            np.matrix(integrated_gradients).sum(axis=1), \n",
    "            pred_y+2, ds.test_labels[i]+2, word_counts)\n",
    "\n",
    "    text = \"</br>\" + pred_text + \"</br>\" + html_text\n",
    "    \n",
    "    if (pred_y == ds.test_labels[i]):\n",
    "        if (pred_y >= 2):\n",
    "            corrHigh.append(text)\n",
    "        else:\n",
    "            corrLow.append(text)\n",
    "    else:\n",
    "        if (ds.test_labels[i] >= 2):\n",
    "            incorrHigh.append(text)\n",
    "        else:\n",
    "            incorrLow.append(text)\n",
    "    \n",
    "    if (i % 1000) == 0:\n",
    "        corrHigh.write(i)\n",
    "        corrLow.write(i)\n",
    "        incorrHigh.write(i)\n",
    "        incorrLow.write(i)\n",
    "        \n",
    "        word_counts.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sensitivity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "#prefix = \"jeff go to supermarket and buy oranges for dinner: \"\n",
    "#prefix = \"we get on a taxi and go to times square in the morning: \"\n",
    "#prefix = \"we were in NYC last week and stay at Hilton.  The service there was great. : \"\n",
    "#prefix = \"we visited my mom and she was not there and we end up going to spa : \"\n",
    "prefix = \"%%%%%%% : \"\n",
    "\n",
    "\n",
    "prefix_tokens = word_tokenize(prefix.lower())\n",
    "\n",
    "# Function to add prefix to input features \n",
    "def add_prefix(features):   \n",
    "    new_features = ds.vocab.words_to_ids(utils.canonicalize_word(w, ds.vocab) for w in prefix_tokens)\n",
    "    new_features.extend(features)\n",
    "    return new_features[0:input_length]\n",
    "\n",
    "    \n",
    "sensitivity_batches = 100  \n",
    "sensitivity_sample_size = train_params['batch_size'] * sensitivity_batches\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# check the accuracy of the sample before adding the prefix \n",
    "# ---------------------------------------------------------\n",
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_test_features, ds.test_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "    if test_batches == sensitivity_batches:\n",
    "        break\n",
    "    \n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Sensitivity test accuracy before adding prefix:{}\".format(test_accuracy))\n",
    "\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# check the accuracy of the sample before adding the prefix \n",
    "# ---------------------------------------------------------\n",
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "# Add prefix to the user reviews\n",
    "\n",
    "sensitivity_features = []\n",
    "sensitivity_labels = ds.test_labels[:sensitivity_sample_size]\n",
    "for i in range(sensitivity_sample_size):\n",
    "    sensitivity_features.append(add_prefix(ds.padded_test_features[i]))\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        sensitivity_features, sensitivity_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "\n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Sensitivity test accuracy after adding prefix:{}\".format(test_accuracy))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Visualize the attributions on user reviewers with added prefix \n",
    "# --------------------------------------------------------------\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    integrated_gradients, pred_y, pred_proba, empty_pred_proba = \\\n",
    "                                get_integrated_gradients_new(sess, sess.graph, sensitivity_features[i])\n",
    "   \n",
    "    print(np.matrix(integrated_gradients).sum())\n",
    "    print(pred_proba[0][pred_y])\n",
    "    print(empty_pred_proba[0][pred_y])\n",
    "\n",
    "    pred_text = \"predicted = {}, actual = {}\".format(pred_y+2, sensitivity_labels[i]+2)\n",
    "    print(\"{}, {}\".format(i, pred_text))\n",
    "\n",
    "    #visualization.visualize_token_attrs(ds.vocab.ids_to_words(ds.test_features[i])[:input_length], np.matrix(gradients).sum(axis=1))\n",
    "    html_text = visualization.visualize_token_attrs(\n",
    "            ds.vocab.ids_to_words(sensitivity_features[i])[:input_length], \n",
    "            np.matrix(integrated_gradients).sum(axis=1), \n",
    "            pred_y+2, sensitivity_labels[i]+2)\n",
    "\n",
    "    text = \"</br>\" + pred_text + \"</br>\" + html_text\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
