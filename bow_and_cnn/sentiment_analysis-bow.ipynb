{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the BOW Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt \n",
    "import glob\n",
    "from importlib import reload\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from common import utils, constants, spell\n",
    "\n",
    "import tensorflow as tf\n",
    "import tripadvisor_ds\n",
    "import visualization\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load tripadvisor dataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reload(tripadvisor_ds)\n",
    "\n",
    "input_length = 500\n",
    "max_bytes = 2**31 - 1\n",
    "\n",
    "data_file = 'data/tripadvisor_ds.pkl'\n",
    "\n",
    "if os.path.isfile(data_file):\n",
    "\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(data_file)\n",
    "    with open(data_file, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    ds = pickle.loads(bytes_in)\n",
    "        \n",
    "else:\n",
    "    ds = tripadvisor_ds.TripAdvisor_DS().process(input_length=input_length)\n",
    "    ds.save(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    413463\n",
       "2    231827\n",
       "1    106079\n",
       "0     90053\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame({'rating': ds.train_labels})\n",
    "labels.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set model parameters \n",
    "model_params = dict(V=ds.vocab.size, \n",
    "                    embed_dim=100, \n",
    "                    num_classes=len(ds.target_labels),\n",
    "                    encoder_type='bow', \n",
    "                    hidden_dims=[1024, 64], \n",
    "                    input_length=input_length,\n",
    "                    lr=0.0001, \n",
    "                    optimizer='adam', \n",
    "                    beta=0.0001)\n",
    "                    \n",
    "train_params = dict(batch_size=64, \n",
    "                    total_epochs=20, \n",
    "                    eval_every=2)\n",
    "\n",
    "\n",
    "summary_params = dict(chkpt_dir=\"./tmp/266_cnn_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup tensorboard \n",
    "\n",
    "if os.path.isdir(summary_params['chkpt_dir']):\n",
    "    shutil.rmtree(summary_params['chkpt_dir'])\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(\"W_embed\", shape=[V, embed_dim], \\\n",
    "                               initializer=tf.random_uniform_initializer(-init_scale, init_scale), \\\n",
    "                               trainable=True)\n",
    "    \n",
    "    #W_embed_ = tf.constant(embedding_matrix, dtype=tf.float32, name=\"W_embed\")\n",
    "    \n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_, name=\"embed_x\")\n",
    "        \n",
    "    return xs_\n",
    "\n",
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.nn.relu,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation, name=(\"Hidden_%d\"%i))\n",
    "        if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(h_, rate=dropout_rate, training=is_training )\n",
    "            h_ = tf.layers.dropout(h_, training=is_training )\n",
    "    return h_\n",
    "\n",
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    W_out_ = tf.get_variable(\"W_out\",  shape=[h_.get_shape().as_list()[1], num_classes], \\\n",
    "                               initializer=tf.random_normal_initializer())\n",
    "    b_out_ = tf.get_variable(\"b_out\", shape=[num_classes])\n",
    "\n",
    "    logits_ = tf.add(tf.matmul(h_, W_out_), b_out_)\n",
    "        \n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "    \n",
    "    with tf.variable_scope(\"Softmax_Layer\"):\n",
    "\n",
    "        softmax_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_, logits=logits_)\n",
    "        \n",
    "        loss_ = tf.reduce_mean(softmax_)\n",
    "    \n",
    "    return loss_, logits_\n",
    "\n",
    "def BOW(ids_, V, embed_dim, hidden_dims, dropout_rate=0, is_training=None):\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "     \n",
    "    sum_xs_ = tf.reduce_sum(xs_, 1)\n",
    "\n",
    "    h_ = fully_connected_layers(sum_xs_, hidden_dims, \\\n",
    "                           dropout_rate=dropout_rate, is_training=is_training)\n",
    "    return h_, xs_\n",
    "\n",
    "\n",
    "def conv_net(ids_, V, embed_dim, filter_sizes, num_filters, hidden_dims, input_length, dropout_rate=0, is_training=None):\n",
    "\n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "\n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim)\n",
    "\n",
    "    xs_ = tf.expand_dims(xs_, -1)\n",
    "        \n",
    "    pooled_outputs_ = []\n",
    "    for _, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"Conv_MaxPool_%d\"%filter_size):\n",
    "            \n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, embed_dim, 1, num_filters]\n",
    "            W_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b_ = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            conv_ = tf.nn.conv2d(\n",
    "                xs_,\n",
    "                W_,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Activation\n",
    "            h_ = tf.nn.relu(tf.nn.bias_add(conv_, b_), name=\"relu\")\n",
    "            \n",
    "            # Maxpooling \n",
    "            pooled_ = tf.nn.max_pool(\n",
    "                h_,\n",
    "                ksize=[1, input_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs_.append(pooled_)\n",
    "            \n",
    "            variable_summaries(pooled_)\n",
    "\n",
    "    # Combine all the pooled features and flatten it\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_ = tf.concat(pooled_outputs_, 3)\n",
    "    h_ = tf.reshape(h_, [-1, num_filters_total])\n",
    "    \n",
    "    # fully connected layers\n",
    "    with tf.variable_scope(\"FC_Layer\"):\n",
    "        h_ = fully_connected_layers(h_, hidden_dims, is_training = is_training)\n",
    "\n",
    "    return h_, xs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, input_length], name='input_x')\n",
    "Y = tf.placeholder(tf.int32, [None,], name='input_y')\n",
    "    \n",
    "\n",
    "if Y is None:\n",
    "    one_hot_y_ = None\n",
    "else:\n",
    "    one_hot_y_ = tf.one_hot(Y, model_params['num_classes'])\n",
    "\n",
    "\n",
    "if model_params['encoder_type'] == 'cnn':\n",
    "    h_, xs_ = conv_net(X, model_params['V'], \n",
    "                      model_params['embed_dim'], \n",
    "                      model_params['filter_sizes'], \n",
    "                      model_params['num_filters'], \n",
    "                      model_params['hidden_dims'],\n",
    "                      model_params['input_length'],\n",
    "                      is_training=True)\n",
    "elif model_params['encoder_type'] == 'bow':\n",
    "    h_, xs_ = BOW(X, model_params['V'], \n",
    "                      model_params['embed_dim'],  \n",
    "                      model_params['hidden_dims'],\n",
    "                      is_training=True)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    ce_loss_, logits_ = softmax_output_layer(h_, Y, model_params['num_classes'])\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "with tf.variable_scope(\"Regularization\"):\n",
    "    l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "    for var_ in tf.trainable_variables():\n",
    "        if \"Embedding_Layer\" in var_.name:\n",
    "            continue\n",
    "        l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "    l2_penalty_ *= model_params['beta']  # scale by regularization strength\n",
    "    tf.summary.scalar('l2_penalty', l2_penalty_)\n",
    "    regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "    tf.summary.scalar('regularized_loss', regularized_loss_)\n",
    "\n",
    "with tf.variable_scope(\"Training\"):\n",
    "    if model_params['optimizer'] == 'adagrad':\n",
    "        optimizer_ = tf.train.AdagradOptimizer(model_params['lr'])\n",
    "    elif  model_params['optimizer'] == 'adam':\n",
    "        optimizer_ = tf.train.AdamOptimizer(model_params['lr'])\n",
    "    else:\n",
    "        optimizer_ = tf.train.GradientDescentOptimizer(model_params['lr'])\n",
    "    train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                    global_step=tf.train.get_global_step())\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    cross_entropy_loss_ = tf.metrics.mean(ce_loss_)\n",
    "    \n",
    "    correct_pred_ = tf.equal(tf.argmax(logits_, 1), tf.argmax(one_hot_y_, 1))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_pred_, tf.float32))\n",
    "\n",
    "    eval_metrics = {\"cross_entropy_loss\": cross_entropy_loss_, \"accuracy\": accuracy_}\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy', ce_loss_)\n",
    "    tf.summary.scalar('accuracy', accuracy_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=./tmp/266_cnn_20181208-0706/train\n",
      "tensorboard --logdir=./tmp/266_cnn_20181208-0706/test\n",
      "1,600 examples, moving-average loss 1.18, train accuracy 0.44\n",
      "3,200 examples, moving-average loss 1.11, train accuracy 0.49\n",
      "4,800 examples, moving-average loss 0.97, train accuracy 0.52\n",
      "6,400 examples, moving-average loss 0.90, train accuracy 0.54\n",
      "8,000 examples, moving-average loss 0.87, train accuracy 0.56\n",
      "9,600 examples, moving-average loss 0.82, train accuracy 0.58\n",
      "11,200 examples, moving-average loss 0.77, train accuracy 0.59\n",
      "12,800 examples, moving-average loss 0.77, train accuracy 0.60\n",
      "14,400 examples, moving-average loss 0.75, train accuracy 0.61\n",
      "16,000 examples, moving-average loss 0.75, train accuracy 0.62\n",
      "17,600 examples, moving-average loss 0.76, train accuracy 0.62\n",
      "19,200 examples, moving-average loss 0.75, train accuracy 0.63\n",
      "20,800 examples, moving-average loss 0.77, train accuracy 0.63\n",
      "22,400 examples, moving-average loss 0.77, train accuracy 0.63\n",
      "24,000 examples, moving-average loss 0.76, train accuracy 0.64\n",
      "25,600 examples, moving-average loss 0.74, train accuracy 0.64\n",
      "27,200 examples, moving-average loss 0.76, train accuracy 0.64\n",
      "28,800 examples, moving-average loss 0.73, train accuracy 0.64\n",
      "30,400 examples, moving-average loss 0.74, train accuracy 0.65\n",
      "32,000 examples, moving-average loss 0.72, train accuracy 0.65\n",
      "33,600 examples, moving-average loss 0.72, train accuracy 0.65\n",
      "35,200 examples, moving-average loss 0.74, train accuracy 0.65\n",
      "36,800 examples, moving-average loss 0.74, train accuracy 0.65\n",
      "38,400 examples, moving-average loss 0.70, train accuracy 0.65\n",
      "40,000 examples, moving-average loss 0.70, train accuracy 0.66\n",
      "41,600 examples, moving-average loss 0.75, train accuracy 0.66\n",
      "43,200 examples, moving-average loss 0.75, train accuracy 0.66\n",
      "44,800 examples, moving-average loss 0.74, train accuracy 0.66\n",
      "46,400 examples, moving-average loss 0.68, train accuracy 0.66\n",
      "48,000 examples, moving-average loss 0.69, train accuracy 0.66\n",
      "49,600 examples, moving-average loss 0.71, train accuracy 0.66\n",
      "51,200 examples, moving-average loss 0.69, train accuracy 0.67\n",
      "52,800 examples, moving-average loss 0.74, train accuracy 0.67\n",
      "54,400 examples, moving-average loss 0.74, train accuracy 0.67\n",
      "56,000 examples, moving-average loss 0.72, train accuracy 0.67\n",
      "57,600 examples, moving-average loss 0.68, train accuracy 0.67\n",
      "59,200 examples, moving-average loss 0.73, train accuracy 0.67\n",
      "60,800 examples, moving-average loss 0.71, train accuracy 0.67\n",
      "62,400 examples, moving-average loss 0.72, train accuracy 0.67\n",
      "64,000 examples, moving-average loss 0.73, train accuracy 0.67\n",
      "65,600 examples, moving-average loss 0.70, train accuracy 0.67\n",
      "67,200 examples, moving-average loss 0.68, train accuracy 0.67\n",
      "68,800 examples, moving-average loss 0.68, train accuracy 0.67\n",
      "70,400 examples, moving-average loss 0.72, train accuracy 0.67\n",
      "72,000 examples, moving-average loss 0.68, train accuracy 0.68\n",
      "73,600 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "75,200 examples, moving-average loss 0.73, train accuracy 0.68\n",
      "76,800 examples, moving-average loss 0.68, train accuracy 0.68\n",
      "78,400 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "80,000 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "81,600 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "83,200 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "84,800 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "86,400 examples, moving-average loss 0.72, train accuracy 0.68\n",
      "88,000 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "89,600 examples, moving-average loss 0.65, train accuracy 0.68\n",
      "91,200 examples, moving-average loss 0.64, train accuracy 0.68\n",
      "92,800 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "94,400 examples, moving-average loss 0.74, train accuracy 0.68\n",
      "96,000 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "97,600 examples, moving-average loss 0.73, train accuracy 0.68\n",
      "99,200 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "100,800 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "102,400 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "104,000 examples, moving-average loss 0.68, train accuracy 0.68\n",
      "105,600 examples, moving-average loss 0.73, train accuracy 0.68\n",
      "107,200 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "108,800 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "110,400 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "112,000 examples, moving-average loss 0.73, train accuracy 0.68\n",
      "113,600 examples, moving-average loss 0.72, train accuracy 0.68\n",
      "115,200 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "116,800 examples, moving-average loss 0.69, train accuracy 0.68\n",
      "118,400 examples, moving-average loss 0.70, train accuracy 0.68\n",
      "120,000 examples, moving-average loss 0.71, train accuracy 0.68\n",
      "121,600 examples, moving-average loss 0.68, train accuracy 0.68\n",
      "123,200 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "124,800 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "126,400 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "128,000 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "129,600 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "131,200 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "132,800 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "134,400 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "136,000 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "137,600 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "139,200 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "140,800 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "142,400 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "144,000 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "145,600 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "147,200 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "148,800 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "150,400 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "152,000 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "153,600 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "155,200 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "156,800 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "158,400 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "160,000 examples, moving-average loss 0.64, train accuracy 0.69\n",
      "161,600 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "163,200 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "164,800 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "166,400 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "168,000 examples, moving-average loss 0.74, train accuracy 0.69\n",
      "169,600 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "171,200 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "172,800 examples, moving-average loss 0.73, train accuracy 0.69\n",
      "174,400 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "176,000 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "177,600 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "179,200 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "180,800 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "182,400 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "184,000 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "185,600 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "187,200 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "188,800 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "190,400 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "192,000 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "193,600 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "195,200 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "196,800 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "198,400 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "200,000 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "201,600 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "203,200 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "204,800 examples, moving-average loss 0.67, train accuracy 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206,400 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "208,000 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "209,600 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "211,200 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "212,800 examples, moving-average loss 0.65, train accuracy 0.69\n",
      "214,400 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "216,000 examples, moving-average loss 0.72, train accuracy 0.69\n",
      "217,600 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "219,200 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "220,800 examples, moving-average loss 0.68, train accuracy 0.69\n",
      "222,400 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "224,000 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "225,600 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "227,200 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "228,800 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "230,400 examples, moving-average loss 0.70, train accuracy 0.69\n",
      "232,000 examples, moving-average loss 0.66, train accuracy 0.69\n",
      "233,600 examples, moving-average loss 0.67, train accuracy 0.69\n",
      "235,200 examples, moving-average loss 0.71, train accuracy 0.69\n",
      "236,800 examples, moving-average loss 0.69, train accuracy 0.69\n",
      "238,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "240,000 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "241,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "243,200 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "244,800 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "246,400 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "248,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "249,600 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "251,200 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "252,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "254,400 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "256,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "257,600 examples, moving-average loss 0.72, train accuracy 0.70\n",
      "259,200 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "260,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "262,400 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "264,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "265,600 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "267,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "268,800 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "270,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "272,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "273,600 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "275,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "276,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "278,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "280,000 examples, moving-average loss 0.72, train accuracy 0.70\n",
      "281,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "283,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "284,800 examples, moving-average loss 0.73, train accuracy 0.70\n",
      "286,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "288,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "289,600 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "291,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "292,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "294,400 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "296,000 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "297,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "299,200 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "300,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "302,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "304,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "305,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "307,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "308,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "310,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "312,000 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "313,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "315,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "316,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "318,400 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "320,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "321,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "323,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "324,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "326,400 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "328,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "329,600 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "331,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "332,800 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "334,400 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "336,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "337,600 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "339,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "340,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "342,400 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "344,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "345,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "347,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "348,800 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "350,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "352,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "353,600 examples, moving-average loss 0.73, train accuracy 0.70\n",
      "355,200 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "356,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "358,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "360,000 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "361,600 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "363,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "364,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "366,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "368,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "369,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "371,200 examples, moving-average loss 0.72, train accuracy 0.70\n",
      "372,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "374,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "376,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "377,600 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "379,200 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "380,800 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "382,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "384,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "385,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "387,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "388,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "390,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "392,000 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "393,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "395,200 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "396,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "398,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "400,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "401,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "403,200 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "404,800 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "406,400 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "408,000 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "409,600 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "411,200 examples, moving-average loss 0.64, train accuracy 0.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412,800 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "414,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "416,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "417,600 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "419,200 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "420,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "422,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "424,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "425,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "427,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "428,800 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "430,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "432,000 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "433,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "435,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "436,800 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "438,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "440,000 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "441,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "443,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "444,800 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "446,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "448,000 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "449,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "451,200 examples, moving-average loss 0.72, train accuracy 0.70\n",
      "452,800 examples, moving-average loss 0.72, train accuracy 0.70\n",
      "454,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "456,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "457,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "459,200 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "460,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "462,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "464,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "465,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "467,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "468,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "470,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "472,000 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "473,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "475,200 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "476,800 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "478,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "480,000 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "481,600 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "483,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "484,800 examples, moving-average loss 0.62, train accuracy 0.70\n",
      "486,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "488,000 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "489,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "491,200 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "492,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "494,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "496,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "497,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "499,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "500,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "502,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "504,000 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "505,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "507,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "508,800 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "510,400 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "512,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "513,600 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "515,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "516,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "518,400 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "520,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "521,600 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "523,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "524,800 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "526,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "528,000 examples, moving-average loss 0.63, train accuracy 0.70\n",
      "529,600 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "531,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "532,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "534,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "536,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "537,600 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "539,200 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "540,800 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "542,400 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "544,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "545,600 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "547,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "548,800 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "550,400 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "552,000 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "553,600 examples, moving-average loss 0.70, train accuracy 0.70\n",
      "555,200 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "556,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "558,400 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "560,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "561,600 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "563,200 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "564,800 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "566,400 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "568,000 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "569,600 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "571,200 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "572,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "574,400 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "576,000 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "577,600 examples, moving-average loss 0.71, train accuracy 0.70\n",
      "579,200 examples, moving-average loss 0.66, train accuracy 0.70\n",
      "580,800 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "582,400 examples, moving-average loss 0.67, train accuracy 0.70\n",
      "584,000 examples, moving-average loss 0.65, train accuracy 0.70\n",
      "585,600 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "587,200 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "588,800 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "590,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "592,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "593,600 examples, moving-average loss 0.69, train accuracy 0.70\n",
      "595,200 examples, moving-average loss 0.68, train accuracy 0.70\n",
      "596,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "598,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "600,000 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "601,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "603,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "604,800 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "606,400 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "608,000 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "609,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "611,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "612,800 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "614,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "616,000 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "617,600 examples, moving-average loss 0.68, train accuracy 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "620,800 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "622,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "624,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "625,600 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "627,200 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "628,800 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "630,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "632,000 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "633,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "635,200 examples, moving-average loss 0.62, train accuracy 0.71\n",
      "636,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "638,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "640,000 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "641,600 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "643,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "644,800 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "646,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "648,000 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "649,600 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "651,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "652,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "654,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "656,000 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "657,600 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "659,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "660,800 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "662,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "664,000 examples, moving-average loss 0.71, train accuracy 0.71\n",
      "665,600 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "667,200 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "668,800 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "670,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "672,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "673,600 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "675,200 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "676,800 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "678,400 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "680,000 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "681,600 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "683,200 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "684,800 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "686,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "688,000 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "689,600 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "691,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "692,800 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "694,400 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "696,000 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "697,600 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "699,200 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "700,800 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "702,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "704,000 examples, moving-average loss 0.71, train accuracy 0.71\n",
      "705,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "707,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "708,800 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "710,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "712,000 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "713,600 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "715,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "716,800 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "718,400 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "720,000 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "721,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "723,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "724,800 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "726,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "728,000 examples, moving-average loss 0.71, train accuracy 0.71\n",
      "729,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "731,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "732,800 examples, moving-average loss 0.73, train accuracy 0.71\n",
      "734,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "736,000 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "737,600 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "739,200 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "740,800 examples, moving-average loss 0.62, train accuracy 0.71\n",
      "742,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "744,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "745,600 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "747,200 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "748,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "750,400 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "752,000 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "753,600 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "755,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "756,800 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "758,400 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "760,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "761,600 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "763,200 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "764,800 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "766,400 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "768,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "769,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "771,200 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "772,800 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "774,400 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "776,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "777,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "779,200 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "780,800 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "782,400 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "784,000 examples, moving-average loss 0.61, train accuracy 0.71\n",
      "785,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "787,200 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "788,800 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "790,400 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "792,000 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "793,600 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "795,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "796,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "798,400 examples, moving-average loss 0.70, train accuracy 0.71\n",
      "800,000 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "801,600 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "803,200 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "804,800 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "806,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "808,000 examples, moving-average loss 0.62, train accuracy 0.71\n",
      "809,600 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "811,200 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "812,800 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "814,400 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "816,000 examples, moving-average loss 0.68, train accuracy 0.71\n",
      "817,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "819,200 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "820,800 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "822,400 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "824,000 examples, moving-average loss 0.67, train accuracy 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "827,200 examples, moving-average loss 0.69, train accuracy 0.71\n",
      "828,800 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "830,400 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "832,000 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "833,600 examples, moving-average loss 0.66, train accuracy 0.71\n",
      "835,200 examples, moving-average loss 0.62, train accuracy 0.71\n",
      "836,800 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "838,400 examples, moving-average loss 0.64, train accuracy 0.71\n",
      "840,000 examples, moving-average loss 0.61, train accuracy 0.71\n",
      "Completed 0 epoch in 0:14:06\n",
      "Train accurary:0.70828\n",
      "Validate accuracy:0.71838\n",
      "841,550 examples, moving-average loss 0.68, train accuracy 0.47\n",
      "843,150 examples, moving-average loss 0.65, train accuracy 0.68\n",
      "844,750 examples, moving-average loss 0.64, train accuracy 0.70\n",
      "846,350 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "847,950 examples, moving-average loss 0.63, train accuracy 0.71\n",
      "849,550 examples, moving-average loss 0.65, train accuracy 0.71\n",
      "851,150 examples, moving-average loss 0.67, train accuracy 0.71\n",
      "852,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "854,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "855,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "857,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "859,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "860,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "862,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "863,950 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "865,550 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "867,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "868,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "870,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "871,950 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "873,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "875,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "876,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "878,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "879,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "881,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "883,150 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "884,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "886,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "887,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "889,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "891,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "892,750 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "894,350 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "895,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "897,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "899,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "900,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "902,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "903,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "905,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "907,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "908,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "910,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "911,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "913,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "915,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "916,750 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "918,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "919,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "921,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "923,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "924,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "926,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "927,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "929,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "931,150 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "932,750 examples, moving-average loss 0.59, train accuracy 0.72\n",
      "934,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "935,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "937,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "939,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "940,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "942,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "943,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "945,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "947,150 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "948,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "950,350 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "951,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "953,550 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "955,150 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "956,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "958,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "959,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "961,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "963,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "964,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "966,350 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "967,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "969,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "971,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "972,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "974,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "975,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "977,550 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "979,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "980,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "982,350 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "983,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "985,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "987,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "988,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "990,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "991,950 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "993,550 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "995,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "996,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "998,350 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "999,950 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,001,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,003,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,004,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,006,350 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,007,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,009,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,011,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,012,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,014,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,015,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,017,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,019,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,020,750 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,022,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,023,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,025,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,027,150 examples, moving-average loss 0.65, train accuracy 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,028,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,030,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,031,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,033,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,035,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,036,750 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,038,350 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "1,039,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,041,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,043,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,044,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,046,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,047,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,049,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,051,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,052,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,054,350 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,055,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,057,550 examples, moving-average loss 0.69, train accuracy 0.72\n",
      "1,059,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,060,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,062,350 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,063,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,065,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,067,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,068,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,070,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,071,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,073,550 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,075,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,076,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,078,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,079,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,081,550 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,083,150 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,084,750 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,086,350 examples, moving-average loss 0.69, train accuracy 0.72\n",
      "1,087,950 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,089,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,091,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,092,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,094,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,095,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,097,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,099,150 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,100,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,102,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,103,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,105,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,107,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,108,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,110,350 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,111,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,113,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,115,150 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,116,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,118,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,119,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,121,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,123,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,124,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,126,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,127,950 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,129,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,131,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,132,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,134,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,135,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,137,550 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,139,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,140,750 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,142,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,143,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,145,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,147,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,148,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,150,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,151,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,153,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,155,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,156,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,158,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,159,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,161,550 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,163,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,164,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,166,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,167,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,169,550 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,171,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,172,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,174,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,175,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,177,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,179,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,180,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,182,350 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,183,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,185,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,187,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,188,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,190,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,191,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,193,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,195,150 examples, moving-average loss 0.68, train accuracy 0.72\n",
      "1,196,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,198,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,199,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,201,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,203,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,204,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,206,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,207,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,209,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,211,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,212,750 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,214,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,215,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,217,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,219,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,220,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,222,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,223,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,225,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,227,150 examples, moving-average loss 0.63, train accuracy 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,228,750 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,230,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,231,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,233,550 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,235,150 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,236,750 examples, moving-average loss 0.59, train accuracy 0.72\n",
      "1,238,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,239,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,241,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,243,150 examples, moving-average loss 0.59, train accuracy 0.72\n",
      "1,244,750 examples, moving-average loss 0.59, train accuracy 0.72\n",
      "1,246,350 examples, moving-average loss 0.67, train accuracy 0.72\n",
      "1,247,950 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,249,550 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,251,150 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,252,750 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,254,350 examples, moving-average loss 0.68, train accuracy 0.73\n",
      "1,255,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,257,550 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,259,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,260,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,262,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,263,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,265,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,267,150 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,268,750 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,270,350 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,271,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,273,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,275,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,276,750 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,278,350 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,279,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,281,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,283,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,284,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,286,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,287,950 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,289,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,291,150 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,292,750 examples, moving-average loss 0.69, train accuracy 0.73\n",
      "1,294,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,295,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,297,550 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,299,150 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,300,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,302,350 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,303,950 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,305,550 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,307,150 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,308,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,310,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,311,950 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,313,550 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,315,150 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,316,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,318,350 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,319,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,321,550 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,323,150 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,324,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,326,350 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,327,950 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,329,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,331,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,332,750 examples, moving-average loss 0.70, train accuracy 0.73\n",
      "1,334,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,335,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,337,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,339,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,340,750 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,342,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,343,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,345,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,347,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,348,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,350,350 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,351,950 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,353,550 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,355,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,356,750 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,358,350 examples, moving-average loss 0.65, train accuracy 0.72\n",
      "1,359,950 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,361,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,363,150 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,364,750 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,366,350 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,367,950 examples, moving-average loss 0.64, train accuracy 0.72\n",
      "1,369,550 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,371,150 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,372,750 examples, moving-average loss 0.66, train accuracy 0.72\n",
      "1,374,350 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,375,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,377,550 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,379,150 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,380,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,382,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,383,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,385,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,387,150 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,388,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,390,350 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,391,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,393,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,395,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,396,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,398,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,399,950 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,401,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,403,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,404,750 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,406,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,407,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,409,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,411,150 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,412,750 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,414,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,415,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,417,550 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,419,150 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,420,750 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,422,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,423,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,425,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,427,150 examples, moving-average loss 0.65, train accuracy 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,428,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,430,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,431,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,433,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,435,150 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,436,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,438,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,439,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,441,550 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,443,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,444,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,446,350 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,447,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,449,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,451,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,452,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,454,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,455,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,457,550 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,459,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,460,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,462,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,463,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,465,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,467,150 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,468,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,470,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,471,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,473,550 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,475,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,476,750 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,478,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,479,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,481,550 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,483,150 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,484,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,486,350 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,487,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,489,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,491,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,492,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,494,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,495,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,497,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,499,150 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,500,750 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,502,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,503,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,505,550 examples, moving-average loss 0.68, train accuracy 0.73\n",
      "1,507,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,508,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,510,350 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,511,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,513,550 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,515,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,516,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,518,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,519,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,521,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,523,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,524,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,526,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,527,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,529,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,531,150 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,532,750 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,534,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,535,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,537,550 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,539,150 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,540,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,542,350 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,543,950 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,545,550 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,547,150 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,548,750 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,550,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,551,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,553,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,555,150 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,556,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,558,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,559,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,561,550 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,563,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,564,750 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,566,350 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,567,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,569,550 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,571,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,572,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,574,350 examples, moving-average loss 0.68, train accuracy 0.73\n",
      "1,575,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,577,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,579,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,580,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,582,350 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,583,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,585,550 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,587,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,588,750 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,590,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,591,950 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,593,550 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,595,150 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,596,750 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,598,350 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,599,950 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,601,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,603,150 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,604,750 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,606,350 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,607,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,609,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,611,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,612,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,614,350 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,615,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,617,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,619,150 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,620,750 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,622,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,623,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,625,550 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,627,150 examples, moving-average loss 0.64, train accuracy 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,628,750 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,630,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,631,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,633,550 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,635,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,636,750 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,638,350 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,639,950 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,641,550 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,643,150 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,644,750 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,646,350 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,647,950 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,649,550 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,651,150 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,652,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,654,350 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,655,950 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,657,550 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,659,150 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,660,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,662,350 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,663,950 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,665,550 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,667,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,668,750 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,670,350 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,671,950 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,673,550 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,675,150 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,676,750 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,678,350 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,679,950 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,681,550 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "Completed 1 epoch in 0:13:59\n",
      "Train accurary:0.72639\n",
      "Validate accuracy:0.72013\n",
      "1,683,100 examples, moving-average loss 0.65, train accuracy 0.57\n",
      "1,684,700 examples, moving-average loss 0.62, train accuracy 0.69\n",
      "1,686,300 examples, moving-average loss 0.62, train accuracy 0.72\n",
      "1,687,900 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,689,500 examples, moving-average loss 0.61, train accuracy 0.72\n",
      "1,691,100 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,692,700 examples, moving-average loss 0.63, train accuracy 0.72\n",
      "1,694,300 examples, moving-average loss 0.60, train accuracy 0.72\n",
      "1,695,900 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,697,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,699,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,700,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,702,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,703,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,705,500 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,707,100 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,708,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,710,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,711,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,713,500 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,715,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,716,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,718,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,719,900 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,721,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,723,100 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,724,700 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,726,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,727,900 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,729,500 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,731,100 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,732,700 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,734,300 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,735,900 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,737,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,739,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,740,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,742,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,743,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,745,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,747,100 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,748,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,750,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,751,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,753,500 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,755,100 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,756,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,758,300 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,759,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,761,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,763,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,764,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,766,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,767,900 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,769,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,771,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,772,700 examples, moving-average loss 0.56, train accuracy 0.73\n",
      "1,774,300 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,775,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,777,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,779,100 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,780,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,782,300 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,783,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,785,500 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,787,100 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,788,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,790,300 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,791,900 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,793,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,795,100 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,796,700 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,798,300 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,799,900 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,801,500 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,803,100 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,804,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,806,300 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,807,900 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,809,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,811,100 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,812,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,814,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,815,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,817,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,819,100 examples, moving-average loss 0.67, train accuracy 0.73\n",
      "1,820,700 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,822,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,823,900 examples, moving-average loss 0.60, train accuracy 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,825,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,827,100 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,828,700 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,830,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,831,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,833,500 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,835,100 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,836,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,838,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,839,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,841,500 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,843,100 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,844,700 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,846,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,847,900 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,849,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,851,100 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,852,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,854,300 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,855,900 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,857,500 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,859,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,860,700 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,862,300 examples, moving-average loss 0.56, train accuracy 0.73\n",
      "1,863,900 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,865,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,867,100 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,868,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,870,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,871,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,873,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,875,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,876,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,878,300 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,879,900 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,881,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,883,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,884,700 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,886,300 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,887,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,889,500 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,891,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,892,700 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,894,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,895,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,897,500 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,899,100 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,900,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,902,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,903,900 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,905,500 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,907,100 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,908,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,910,300 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,911,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,913,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,915,100 examples, moving-average loss 0.57, train accuracy 0.73\n",
      "1,916,700 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,918,300 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,919,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,921,500 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,923,100 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,924,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,926,300 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,927,900 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,929,500 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,931,100 examples, moving-average loss 0.65, train accuracy 0.73\n",
      "1,932,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,934,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,935,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,937,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,939,100 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,940,700 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,942,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,943,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,945,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,947,100 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,948,700 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,950,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,951,900 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,953,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,955,100 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,956,700 examples, moving-average loss 0.58, train accuracy 0.73\n",
      "1,958,300 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,959,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,961,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,963,100 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,964,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,966,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,967,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,969,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,971,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,972,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,974,300 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,975,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,977,500 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,979,100 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,980,700 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,982,300 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,983,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,985,500 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "1,987,100 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "1,988,700 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,990,300 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "1,991,900 examples, moving-average loss 0.63, train accuracy 0.73\n",
      "1,993,500 examples, moving-average loss 0.61, train accuracy 0.73\n",
      "1,995,100 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,996,700 examples, moving-average loss 0.60, train accuracy 0.73\n",
      "1,998,300 examples, moving-average loss 0.66, train accuracy 0.73\n",
      "1,999,900 examples, moving-average loss 0.62, train accuracy 0.73\n",
      "2,001,500 examples, moving-average loss 0.64, train accuracy 0.73\n",
      "2,003,100 examples, moving-average loss 0.59, train accuracy 0.73\n",
      "2,004,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,006,300 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,007,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,009,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,011,100 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,012,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,014,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,015,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,017,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,019,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,020,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,022,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,023,900 examples, moving-average loss 0.64, train accuracy 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,025,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,027,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,028,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,030,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,031,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,033,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,035,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,036,700 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,038,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,039,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,041,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,043,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,044,700 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,046,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,047,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,049,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,051,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,052,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,054,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,055,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,057,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,059,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,060,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,062,300 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,063,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,065,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,067,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,068,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,070,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,071,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,073,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,075,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,076,700 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,078,300 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,079,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,081,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,083,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,084,700 examples, moving-average loss 0.56, train accuracy 0.74\n",
      "2,086,300 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,087,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,089,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,091,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,092,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,094,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,095,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,097,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,099,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,100,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,102,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,103,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,105,500 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,107,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,108,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,110,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,111,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,113,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,115,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,116,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,118,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,119,900 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,121,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,123,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,124,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,126,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,127,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,129,500 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,131,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,132,700 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,134,300 examples, moving-average loss 0.67, train accuracy 0.74\n",
      "2,135,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,137,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,139,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,140,700 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,142,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,143,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,145,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,147,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,148,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,150,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,151,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,153,500 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,155,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,156,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,158,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,159,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,161,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,163,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,164,700 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,166,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,167,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,169,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,171,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,172,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,174,300 examples, moving-average loss 0.68, train accuracy 0.74\n",
      "2,175,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,177,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,179,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,180,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,182,300 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,183,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,185,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,187,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,188,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,190,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,191,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,193,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,195,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,196,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,198,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,199,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,201,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,203,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,204,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,206,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,207,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,209,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,211,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,212,700 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,214,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,215,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,217,500 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,219,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,220,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,222,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,223,900 examples, moving-average loss 0.64, train accuracy 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,225,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,227,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,228,700 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,230,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,231,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,233,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,235,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,236,700 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,238,300 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,239,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,241,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,243,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,244,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,246,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,247,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,249,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,251,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,252,700 examples, moving-average loss 0.55, train accuracy 0.74\n",
      "2,254,300 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,255,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,257,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,259,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,260,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,262,300 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,263,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,265,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,267,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,268,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,270,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,271,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,273,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,275,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,276,700 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,278,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,279,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,281,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,283,100 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,284,700 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,286,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,287,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,289,500 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,291,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,292,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,294,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,295,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,297,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,299,100 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,300,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,302,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,303,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,305,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,307,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,308,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,310,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,311,900 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,313,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,315,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,316,700 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,318,300 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,319,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,321,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,323,100 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,324,700 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,326,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,327,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,329,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,331,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,332,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,334,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,335,900 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,337,500 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,339,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,340,700 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,342,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,343,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,345,500 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,347,100 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,348,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,350,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,351,900 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,353,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,355,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,356,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,358,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,359,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,361,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,363,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,364,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,366,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,367,900 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,369,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,371,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,372,700 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,374,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,375,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,377,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,379,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,380,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,382,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,383,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,385,500 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,387,100 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,388,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,390,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,391,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,393,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,395,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,396,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,398,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,399,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,401,500 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,403,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,404,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,406,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,407,900 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,409,500 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,411,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,412,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,414,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,415,900 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,417,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,419,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,420,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,422,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,423,900 examples, moving-average loss 0.60, train accuracy 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,425,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,427,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,428,700 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,430,300 examples, moving-average loss 0.65, train accuracy 0.74\n",
      "2,431,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,433,500 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,435,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,436,700 examples, moving-average loss 0.66, train accuracy 0.74\n",
      "2,438,300 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,439,900 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,441,500 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,443,100 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,444,700 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,446,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,447,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,449,500 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "2,451,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,452,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,454,300 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,455,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,457,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,459,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,460,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,462,300 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,463,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,465,500 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,467,100 examples, moving-average loss 0.55, train accuracy 0.74\n",
      "2,468,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,470,300 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,471,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,473,500 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,475,100 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,476,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,478,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,479,900 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,481,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,483,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,484,700 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,486,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,487,900 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,489,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,491,100 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,492,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,494,300 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,495,900 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,497,500 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,499,100 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,500,700 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,502,300 examples, moving-average loss 0.64, train accuracy 0.74\n",
      "2,503,900 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,505,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,507,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,508,700 examples, moving-average loss 0.63, train accuracy 0.74\n",
      "2,510,300 examples, moving-average loss 0.60, train accuracy 0.74\n",
      "2,511,900 examples, moving-average loss 0.59, train accuracy 0.74\n",
      "2,513,500 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,515,100 examples, moving-average loss 0.61, train accuracy 0.74\n",
      "2,516,700 examples, moving-average loss 0.62, train accuracy 0.74\n",
      "2,518,300 examples, moving-average loss 0.56, train accuracy 0.74\n",
      "2,519,900 examples, moving-average loss 0.57, train accuracy 0.74\n",
      "2,521,500 examples, moving-average loss 0.56, train accuracy 0.74\n",
      "2,523,100 examples, moving-average loss 0.58, train accuracy 0.74\n",
      "Completed 2 epoch in 0:13:53\n",
      "Train accurary:0.73707\n",
      "Validate accuracy:0.71684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# Tensorboard - Visualize graph \n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(summary_params['chkpt_dir'] + '/test')\n",
    "\n",
    "print(\"tensorboard --logdir={}/train\".format(summary_params['chkpt_dir']))\n",
    "print(\"tensorboard --logdir={}/test\".format(summary_params['chkpt_dir']))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "sess.run(init_l)\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(train_params['total_epochs']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_batches = 1\n",
    "    train_accuracy = 0.0\n",
    "    \n",
    "    for (bx, by) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_train_features, ds.train_labels):\n",
    "\n",
    "        summary, batch_loss, _, batch_accuracy = sess.run(\n",
    "            [merged, regularized_loss_, train_op_, accuracy_], feed_dict={X: bx, Y: by})\n",
    "        \n",
    "        train_batches +=1\n",
    "        train_accuracy += batch_accuracy\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}, train accuracy {:.2f}\"\\\n",
    "                  .format(total_examples, loss_ema, train_accuracy/train_batches))    \n",
    "            \n",
    "        train_writer.add_summary(summary, total_batches)\n",
    "\n",
    "    print(\"Completed {} epoch in {:s}\".format(i, utils.pretty_timedelta(since=t0)))\n",
    "    \n",
    "    train_accuracy = train_accuracy/train_batches\n",
    "    print(\"Train accurary:{:.5f}\".format(train_accuracy))\n",
    "    \n",
    "    \n",
    "    # run the validation dataset \n",
    "    validate_batches = 1\n",
    "    validate_accuracy = 0.0\n",
    "    for (vx, vy) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                            ds.padded_validate_features, ds.validate_labels):\n",
    "\n",
    "        summary, batch_accuracy = sess.run([merged, accuracy_], feed_dict={X: vx, Y: vy})\n",
    "\n",
    "        validate_batches +=1\n",
    "        validate_accuracy += batch_accuracy\n",
    "\n",
    "        test_writer.add_summary(summary, total_batches + validate_batches)\n",
    "\n",
    "    validate_accuracy = validate_accuracy/validate_batches\n",
    "    print(\"Validate accuracy:{:.5f}\".format(validate_accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.7110148514851485\n"
     ]
    }
   ],
   "source": [
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_test_features, ds.test_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Test accuracy:{}\".format(test_accuracy))\n",
    "\n",
    "pred_y = [y for x in test_pred_y for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8244,  2245,   367,   100],\n",
       "       [ 1902,  6760,  3952,   417],\n",
       "       [  256,  2283, 16288,  9850],\n",
       "       [   69,   261,  7482, 43403]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ds.test_labels, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#op = sess.graph.get_operations()\n",
    "#[m.values() for m in op]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Integrated Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_gradients(sess, graph, review):\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        # get prediction \n",
    "        pred_prob_ = graph.get_tensor_by_name('Prediction/pred_proba:0')\n",
    "        pred_prob = sess.run([pred_prob_], feed_dict={X:[review], Y:[0]})\n",
    "        \n",
    "        # get gradient     \n",
    "        input_y_ = graph.get_tensor_by_name('input_y:0')[0]\n",
    "        embed_x_ = graph.get_tensor_by_name('Embedding_Layer/embed_x:0')\n",
    "        label_prediction_ = graph.get_tensor_by_name('Prediction/pred_proba:0')[:, input_y_]\n",
    "        grads_ = tf.gradients(label_prediction_, embed_x_)[0]\n",
    "        \n",
    "        embed_x, grads = sess.run([embed_x_, grads_], feed_dict={X:[review], Y:[np.argmax(pred_prob)]})\n",
    "        return grads * embed_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrated_gradients_new(sess, graph, review, steps = 50):\n",
    "\n",
    "    empty_review = np.zeros(input_length)\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        # get embedding and prediction \n",
    "        embed_x_ = graph.get_tensor_by_name('Embedding_Layer/embed_x:0')\n",
    "        pred_prob_ = graph.get_tensor_by_name('Prediction/pred_proba:0')\n",
    "        embed_x, pred_y, pred_prob = sess.run([embed_x_, pred_max_, pred_prob_], feed_dict={X:[review], Y:[4]})\n",
    "          \n",
    "        # empty embedding \n",
    "        empty_embed_x, empty_pred_prob = sess.run([embed_x_, pred_prob_], feed_dict={X:[empty_review], Y:[4]})\n",
    "            \n",
    "            \n",
    "        # get integrated gradient \n",
    "        input_y_ = graph.get_tensor_by_name('input_y:0')[0]\n",
    "        label_prediction_ = graph.get_tensor_by_name('Prediction/pred_proba:0')[:, input_y_]\n",
    "        grads_ = tf.gradients(label_prediction_, embed_x_)[0]\n",
    "\n",
    "        \n",
    "        all_grads = []\n",
    "        for i in range(0, steps + 1):\n",
    "            _, grads = sess.run([label_prediction_, grads_], feed_dict={embed_x_:(empty_embed_x + (embed_x - empty_embed_x)*(float(i)/steps)), Y:pred_y})\n",
    "            all_grads.append(grads)\n",
    "        \n",
    "        integrated_grads = np.average(all_grads[:-1], axis=0) * embed_x\n",
    "        return integrated_grads, pred_y[0], pred_prob, empty_pred_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 supporing classes to save the output of the Integrated Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttributionResult(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.html_text = \"<html><body>\"\n",
    "    \n",
    "    def append(self, text):\n",
    "        self.html_text += text\n",
    "    \n",
    "    def write(self, index):\n",
    "        self.html_text += \"</body></html>\"\n",
    "        \n",
    "        with open(\"result/{}_{}.html\".format(self.filename, index), \"w\") as the_file:\n",
    "            the_file.write(self.html_text)\n",
    "            \n",
    "        self.html_text = \"<html><body>\"\n",
    "        \n",
    "class AttributionCount(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.word_count = []\n",
    "        \n",
    "    def append(self, token):\n",
    "        self.word_count.append(token)\n",
    "    \n",
    "    def write(self, index):\n",
    "        df = pd.DataFrame.from_records(self.word_count)\n",
    "        df.columns = ['pred', 'target', 'vocab', 'attr']\n",
    "        df.attr.round(5)\n",
    "        df.to_csv(\"result/{}_{}.csv\".format(self.filename, index))\n",
    "    \n",
    "        self.word_count = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize attributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23612717\n",
      "0.46020532\n",
      "0.22236566\n",
      "0, predicted = 3, actual = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <span style='color:rgb(66,66,251)'>beautiful</span> <span style='color:rgb(127,128,128)'>hotel</span> <span style='color:rgb(180,102,102)'>but</span> <span style='color:rgb(127,127,130)'>the</span> <span style='color:rgb(178,103,103)'>housekeeping</span> <span style='color:rgb(130,127,127)'>staff</span> <span style='color:rgb(124,124,136)'><unk></span> <span style='color:rgb(115,115,153)'>your</span> <span style='color:rgb(161,111,111)'>room</span> <span style='color:rgb(123,123,138)'>checked</span> <span style='color:rgb(127,127,129)'>into</span> <span style='color:rgb(114,114,156)'>bally</span> <span style='color:rgb(149,117,117)'>'s</span> <span style='color:rgb(120,120,144)'>las</span> <span style='color:rgb(97,97,189)'>vegas</span> <span style='color:rgb(130,127,127)'>on</span> <span style='color:rgb(110,110,164)'>sunday</span> <span style='color:rgb(143,120,120)'>jan</span> <span style='color:rgb(127,128,128)'>DGDG</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(157,113,113)'>check</span> <span style='color:rgb(126,126,131)'>out</span> <span style='color:rgb(155,114,114)'>wednesday</span> <span style='color:rgb(143,120,120)'>jan</span> <span style='color:rgb(127,128,128)'>DGDG</span> <span style='color:rgb(124,124,135)'>,</span> <span style='color:rgb(130,127,127)'>on</span> <span style='color:rgb(101,101,182)'>monday</span> <span style='color:rgb(129,127,127)'>morning</span> <span style='color:rgb(119,119,145)'>we</span> <span style='color:rgb(145,119,119)'>left</span> <span style='color:rgb(128,128,128)'>our</span> <span style='color:rgb(161,111,111)'>room</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(127,127,130)'>go</span> <span style='color:rgb(130,127,127)'>on</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(148,118,118)'>day</span> <span style='color:rgb(114,114,156)'>trip</span> <span style='color:rgb(125,125,133)'>due</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(82,82,219)'>snow</span> <span style='color:rgb(119,119,145)'>we</span> <span style='color:rgb(131,126,126)'>got</span> <span style='color:rgb(95,95,193)'>snowed</span> <span style='color:rgb(127,127,130)'>in</span> <span style='color:rgb(130,127,127)'>on</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(134,125,125)'>mountain</span> <span style='color:rgb(119,119,146)'>and</span> <span style='color:rgb(224,80,80)'>didnt</span> <span style='color:rgb(114,114,155)'>return</span> <span style='color:rgb(208,88,88)'>til</span> <span style='color:rgb(137,123,123)'>tuesday</span> <span style='color:rgb(113,113,157)'>all</span> <span style='color:rgb(132,126,126)'>of</span> <span style='color:rgb(128,128,128)'>our</span> <span style='color:rgb(121,121,142)'>things</span> <span style='color:rgb(153,115,115)'>were</span> <span style='color:rgb(206,89,89)'>gone</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(178,103,103)'>housekeeping</span> <span style='color:rgb(143,120,120)'>had</span> <span style='color:rgb(147,118,118)'>removed</span> <span style='color:rgb(132,126,126)'>them</span> <span style='color:rgb(124,124,135)'>,</span> <span style='color:rgb(128,128,128)'>so</span> <span style='color:rgb(124,124,136)'>ended</span> <span style='color:rgb(123,123,138)'>up</span> <span style='color:rgb(127,127,130)'>in</span> <span style='color:rgb(124,124,136)'>lost</span> <span style='color:rgb(119,119,146)'>and</span> <span style='color:rgb(122,122,139)'>found</span> <span style='color:rgb(146,119,119)'>others</span> <span style='color:rgb(143,120,120)'>are</span> <span style='color:rgb(132,126,126)'>still</span> <span style='color:rgb(185,99,99)'>mia</span> <span style='color:rgb(124,124,135)'>,</span> <span style='color:rgb(121,121,142)'>security</span> <span style='color:rgb(145,119,119)'>was</span> <span style='color:rgb(65,65,254)'>great</span> <span style='color:rgb(127,127,130)'>the</span> <span style='color:rgb(152,116,116)'>front</span> <span style='color:rgb(163,110,110)'>desk</span> <span style='color:rgb(128,128,127)'>manager</span> <span style='color:rgb(145,119,119)'>was</span> <span style='color:rgb(229,77,77)'>awful</span> <span style='color:rgb(135,124,124)'>jeff</span> <span style='color:rgb(151,116,116)'>would</span> <span style='color:rgb(246,69,69)'>not</span> <span style='color:rgb(124,124,135)'>even</span> <span style='color:rgb(120,120,144)'>bother</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(146,119,119)'>speak</span> <span style='color:rgb(128,128,128)'>with</span> <span style='color:rgb(112,112,159)'>me</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(128,128,128)'>called</span> <span style='color:rgb(115,115,153)'>my</span> <span style='color:rgb(128,128,128)'>host</span> <span style='color:rgb(114,114,156)'>she</span> <span style='color:rgb(145,119,119)'>was</span> <span style='color:rgb(126,126,131)'>out</span> <span style='color:rgb(180,102,102)'>but</span> <span style='color:rgb(178,103,103)'>someone</span> <span style='color:rgb(131,126,126)'>got</span> <span style='color:rgb(121,121,142)'>security</span> <span style='color:rgb(123,123,138)'>up</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(161,111,111)'>room</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(140,122,122)'>when</span> <span style='color:rgb(150,117,117)'>checking</span> <span style='color:rgb(126,126,131)'>out</span> <span style='color:rgb(130,127,127)'>they</span> <span style='color:rgb(132,126,126)'>still</span> <span style='color:rgb(138,123,123)'>wanted</span> <span style='color:rgb(128,128,127)'>to</span> <span style='color:rgb(158,113,113)'>charge</span> <span style='color:rgb(112,112,159)'>me</span> <span style='color:rgb(122,122,139)'>their</span> <span style='color:rgb(125,125,134)'>resort</span> <span style='color:rgb(209,87,87)'>fees</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(128,128,127)'>also</span> <span style='color:rgb(130,127,127)'>they</span> <span style='color:rgb(118,118,148)'>now</span> <span style='color:rgb(158,113,113)'>charge</span> <span style='color:rgb(136,124,124)'>for</span> <span style='color:rgb(126,126,131)'>valet</span> <span style='color:rgb(130,127,127)'>parking</span> <span style='color:rgb(124,124,135)'>,</span> <span style='color:rgb(141,121,121)'>there</span> <span style='color:rgb(137,123,123)'>is</span> <span style='color:rgb(246,69,69)'>not</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(142,121,121)'>daily</span> <span style='color:rgb(119,119,145)'>rate</span> <span style='color:rgb(136,124,124)'>for</span> <span style='color:rgb(137,123,123)'>people</span> <span style='color:rgb(120,120,144)'>staying</span> <span style='color:rgb(127,127,130)'>in</span> <span style='color:rgb(122,122,139)'>their</span> <span style='color:rgb(127,128,128)'>hotel</span> <span style='color:rgb(121,121,142)'>it</span> <span style='color:rgb(137,123,123)'>is</span> <span style='color:rgb(127,128,128)'>DGDG</span> <span style='color:rgb(134,125,125)'>minimum</span> <span style='color:rgb(124,124,136)'><unk></span> <span style='color:rgb(119,119,146)'>and</span> <span style='color:rgb(127,128,128)'>DGDG</span> <span style='color:rgb(136,124,124)'>for</span> <span style='color:rgb(64,64,255)'>above</span> <span style='color:rgb(148,118,118)'>DG</span> <span style='color:rgb(126,126,132)'>hrs</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(128,128,128)'>so</span> <span style='color:rgb(168,108,108)'>if</span> <span style='color:rgb(124,124,136)'>you</span> <span style='color:rgb(120,120,144)'>come</span> <span style='color:rgb(127,127,130)'>in</span> <span style='color:rgb(119,119,146)'>and</span> <span style='color:rgb(126,126,131)'>out</span> <span style='color:rgb(124,124,136)'>you</span> <span style='color:rgb(164,110,110)'>pay</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(152,116,116)'>lot</span> <span style='color:rgb(129,127,127)'>.</span> <span style='color:rgb(122,122,140)'>i</span> <span style='color:rgb(151,116,116)'>would</span> <span style='color:rgb(123,123,138)'>suggest</span> <span style='color:rgb(178,103,103)'>another</span> <span style='color:rgb(127,128,128)'>hotel</span> <span style='color:rgb(129,127,127)'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5733487\n",
      "0.75059676\n",
      "0.36662394\n",
      "1, predicted = 5, actual = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <span style='color:rgb(234,75,75)'>best</span> <span style='color:rgb(126,126,131)'>view</span> <span style='color:rgb(128,128,128)'>in</span> <span style='color:rgb(165,109,109)'>nyc</span> <span style='color:rgb(124,124,135)'>had</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(191,96,96)'>great</span> <span style='color:rgb(150,117,117)'>stay</span> <span style='color:rgb(135,124,124)'>here</span> <span style='color:rgb(128,128,128)'>made</span> <span style='color:rgb(255,64,64)'>perfect</span> <span style='color:rgb(137,123,123)'>by</span> <span style='color:rgb(128,128,127)'>the</span> <span style='color:rgb(126,126,131)'>view</span> <span style='color:rgb(135,124,124)'>from</span> <span style='color:rgb(127,127,129)'>our</span> <span style='color:rgb(122,122,140)'>floor</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(153,115,115)'>ceiling</span> <span style='color:rgb(121,121,141)'>windows</span> <span style='color:rgb(126,126,131)'>on</span> <span style='color:rgb(128,128,127)'>the</span> <span style='color:rgb(169,107,107)'>12th</span> <span style='color:rgb(127,127,130)'><unk></span> <span style='color:rgb(255,64,64)'>perfect</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(100,100,184)'>good</span> <span style='color:rgb(127,128,128)'>size</span> <span style='color:rgb(122,122,139)'>for</span> <span style='color:rgb(157,113,113)'>new</span> <span style='color:rgb(126,126,131)'>york</span> <span style='color:rgb(114,114,155)'>nice</span> <span style='color:rgb(122,122,140)'>decor</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(178,103,103)'>everything</span> <span style='color:rgb(138,123,123)'>you</span> <span style='color:rgb(127,127,130)'><unk></span> <span style='color:rgb(172,106,106)'>ideal</span> <span style='color:rgb(122,122,139)'>for</span> <span style='color:rgb(132,126,126)'>subway</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(126,126,131)'>walking</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(136,124,124)'>midtown</span> <span style='color:rgb(123,123,138)'>or</span> <span style='color:rgb(123,123,137)'>downtown</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(114,114,155)'>nice</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(134,125,125)'>have</span> <span style='color:rgb(127,128,128)'>chinatown</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(116,116,152)'>little</span> <span style='color:rgb(137,123,123)'>italy</span> <span style='color:rgb(126,126,131)'>on</span> <span style='color:rgb(128,128,127)'>the</span> <span style='color:rgb(127,127,130)'><unk></span> <span style='color:rgb(172,106,106)'>recommend</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(137,123,123)'>friends</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(108,108,168)'>would</span> <span style='color:rgb(150,117,117)'>stay</span> <span style='color:rgb(135,124,124)'>here</span> <span style='color:rgb(127,127,130)'><unk></span> <span style='color:rgb(71,71,242)'>disappointment</span> <span style='color:rgb(125,125,133)'>after</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(191,96,96)'>great</span> <span style='color:rgb(150,117,117)'>stay</span> <span style='color:rgb(124,124,136)'>was</span> <span style='color:rgb(121,121,141)'>being</span> <span style='color:rgb(133,125,125)'>asked</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(114,114,156)'>pay</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(180,102,102)'>leave</span> <span style='color:rgb(127,127,129)'>our</span> <span style='color:rgb(132,126,126)'>bags</span> <span style='color:rgb(128,128,128)'>in</span> <span style='color:rgb(111,111,161)'>reception</span> <span style='color:rgb(125,125,133)'>after</span> <span style='color:rgb(150,117,117)'>checkout</span> <span style='color:rgb(128,128,127)'>.</span> <span style='color:rgb(149,117,117)'>never</span> <span style='color:rgb(124,124,135)'>had</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(114,114,156)'>pay</span> <span style='color:rgb(122,122,139)'>for</span> <span style='color:rgb(141,121,121)'>this</span> <span style='color:rgb(139,122,122)'>service</span> <span style='color:rgb(127,127,129)'>before</span> <span style='color:rgb(135,124,124)'>and</span> <span style='color:rgb(131,126,126)'>it</span> <span style='color:rgb(101,101,181)'>seems</span> <span style='color:rgb(115,115,153)'>quite</span> <span style='color:rgb(167,108,108)'>penny</span> <span style='color:rgb(102,102,179)'>pinching</span> <span style='color:rgb(125,125,133)'>after</span> <span style='color:rgb(113,113,158)'>paying</span> <span style='color:rgb(128,128,127)'>around</span> <span style='color:rgb(116,116,152)'>$</span> <span style='color:rgb(142,121,121)'>DGDGDG</span> <span style='color:rgb(122,122,140)'>per</span> <span style='color:rgb(122,122,139)'>night</span> <span style='color:rgb(122,122,139)'>for</span> <span style='color:rgb(127,127,129)'>a</span> <span style='color:rgb(150,117,117)'>stay</span> <span style='color:rgb(128,128,127)'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6436432\n",
      "0.8438525\n",
      "0.36662394\n",
      "2, predicted = 5, actual = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <span style='color:rgb(133,125,125)'>very</span> <span style='color:rgb(117,117,149)'>urban</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(255,64,64)'>fantastic</span> <span style='color:rgb(88,88,208)'>location</span> <span style='color:rgb(203,90,90)'>great</span> <span style='color:rgb(165,109,109)'>new</span> <span style='color:rgb(136,124,124)'>hotel</span> <span style='color:rgb(128,128,128)'>.</span> <span style='color:rgb(73,73,237)'>small</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(155,114,114)'>cozy</span> <span style='color:rgb(119,119,146)'>rooms</span> <span style='color:rgb(93,93,198)'>but</span> <span style='color:rgb(110,110,163)'>nice</span> <span style='color:rgb(151,116,116)'>large</span> <span style='color:rgb(119,119,145)'>bathroom</span> <span style='color:rgb(128,128,128)'>.</span> <span style='color:rgb(229,77,77)'>love</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(119,119,146)'>neighborhood</span> <span style='color:rgb(120,120,143)'>(</span> <span style='color:rgb(141,121,121)'>steps</span> <span style='color:rgb(137,123,123)'>away</span> <span style='color:rgb(137,123,123)'>from</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(159,112,112)'>blue</span> <span style='color:rgb(126,126,132)'>line</span> <span style='color:rgb(123,123,137)'>which</span> <span style='color:rgb(123,123,138)'>goes</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(166,109,109)'>o'hare</span> <span style='color:rgb(145,119,119)'>)</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(143,120,120)'>we</span> <span style='color:rgb(229,77,77)'>love</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(151,116,116)'>feel</span> <span style='color:rgb(124,124,136)'>of</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(152,116,116)'>hotel-we</span> <span style='color:rgb(169,107,107)'>will</span> <span style='color:rgb(204,90,90)'>definitely</span> <span style='color:rgb(158,113,113)'>return</span> <span style='color:rgb(152,116,116)'>!</span> <span style='color:rgb(152,116,116)'>!</span> <span style='color:rgb(152,116,116)'>staff</span> <span style='color:rgb(122,122,140)'>was</span> <span style='color:rgb(133,125,125)'>very</span> <span style='color:rgb(176,104,104)'>accommodating</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(141,121,121)'>friendly</span> <span style='color:rgb(128,128,128)'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7085015\n",
      "0.97652996\n",
      "0.36662394\n",
      "3, predicted = 5, actual = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <span style='color:rgb(148,118,118)'>mother</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(148,118,118)'>daughter</span> <span style='color:rgb(138,123,123)'>lunch</span> <span style='color:rgb(223,80,80)'>wow</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(131,126,126)'>most</span> <span style='color:rgb(255,64,64)'>amazing</span> <span style='color:rgb(127,128,128)'><unk></span> <span style='color:rgb(172,106,106)'>cheese</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(164,110,110)'>tomato</span> <span style='color:rgb(106,106,171)'>soup</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(124,124,136)'>ambience</span> <span style='color:rgb(122,122,139)'>was</span> <span style='color:rgb(134,125,125)'>insanely</span> <span style='color:rgb(206,89,89)'>beautiful</span> <span style='color:rgb(148,118,118)'>!</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(142,121,121)'>service</span> <span style='color:rgb(122,122,139)'>was</span> <span style='color:rgb(254,65,65)'>excellent</span> <span style='color:rgb(137,123,123)'>and</span> <span style='color:rgb(226,79,79)'>everyone</span> <span style='color:rgb(122,122,139)'>was</span> <span style='color:rgb(134,125,125)'>friendly</span> <span style='color:rgb(128,128,127)'>.</span> <span style='color:rgb(117,117,149)'>sometimes</span> <span style='color:rgb(128,128,127)'>places</span> <span style='color:rgb(112,112,160)'>similar</span> <span style='color:rgb(127,128,128)'>to</span> <span style='color:rgb(144,120,120)'>this</span> <span style='color:rgb(145,119,119)'>could</span> <span style='color:rgb(128,128,128)'>be</span> <span style='color:rgb(133,125,125)'>stuffy</span> <span style='color:rgb(133,125,125)'>,</span> <span style='color:rgb(68,68,247)'>however</span> <span style='color:rgb(133,125,125)'>,</span> <span style='color:rgb(102,102,180)'>not</span> <span style='color:rgb(127,128,128)'>the</span> <span style='color:rgb(144,120,120)'>case</span> <span style='color:rgb(132,126,126)'>at</span> <span style='color:rgb(149,117,117)'>all</span> <span style='color:rgb(137,123,123)'>here</span> <span style='color:rgb(128,128,127)'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-12f476f0beeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadded_test_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#integrated_gradients, pred_y, pred_proba = get_integrated_gradients(sess, sess.graph, ds.padded_test_features[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mintegrated_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m                                 \u001b[0mget_integrated_gradients_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadded_test_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7163077cf087>\u001b[0m in \u001b[0;36mget_integrated_gradients_new\u001b[0;34m(sess, graph, review, steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mall_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_prediction_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0membed_x_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_embed_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mempty_embed_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mall_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(visualization)\n",
    "\n",
    "# save the attributions \n",
    "file_suffix = \"bow_d{}_h1024_64_epoch{}\".format(model_params['embed_dim'], \\\n",
    "                                   train_params['total_epochs'])\n",
    "\n",
    "corrHigh = AttributionResult(\"CorrHigh_{}\".format(file_suffix))\n",
    "corrLow = AttributionResult(\"CorrLow_{}\".format(file_suffix))\n",
    "incorrHigh = AttributionResult(\"InCorrHigh_{}\".format(file_suffix))\n",
    "incorrLow = AttributionResult(\"InCorrLow_{}\".format(file_suffix))\n",
    "word_counts = AttributionCount(\"wordcounts_{}\".format(file_suffix))\n",
    "\n",
    "\n",
    "# loop through the test data to visualize the result \n",
    "\n",
    "for i in range(0, len(ds.test_labels)):\n",
    "    gradients = get_gradients(sess, sess.graph, ds.padded_test_features[i])\n",
    "    integrated_gradients, pred_y, pred_proba, empty_pred_proba = \\\n",
    "                                get_integrated_gradients_new(sess, sess.graph, ds.padded_test_features[i])\n",
    "   \n",
    "    \n",
    "    #print(np.matrix(gradients).sum())\n",
    "    print(np.matrix(integrated_gradients).sum())\n",
    "    print(pred_proba[0][pred_y])\n",
    "    print(empty_pred_proba[0][pred_y])\n",
    "\n",
    "    pred_text = \"predicted = {}, actual = {}\".format(pred_y+2, ds.test_labels[i]+2)\n",
    "    print(\"{}, {}\".format(i, pred_text))\n",
    "\n",
    "    html_text = visualization.visualize_token_attrs(\n",
    "            ds.vocab.ids_to_words(ds.test_features[i])[:input_length], \n",
    "            np.matrix(integrated_gradients).sum(axis=1), \n",
    "            pred_y+2, ds.test_labels[i]+2, word_counts)\n",
    "\n",
    "    text = \"</br>\" + pred_text + \"</br>\" + html_text\n",
    "    \n",
    "    if (pred_y == ds.test_labels[i]):\n",
    "        if (pred_y >= 2):\n",
    "            corrHigh.append(text)\n",
    "        else:\n",
    "            corrLow.append(text)\n",
    "    else:\n",
    "        if (ds.test_labels[i] >= 2):\n",
    "            incorrHigh.append(text)\n",
    "        else:\n",
    "            incorrLow.append(text)\n",
    "    \n",
    "    if (i % 1000) == 0:\n",
    "        corrHigh.write(i)\n",
    "        corrLow.write(i)\n",
    "        incorrHigh.write(i)\n",
    "        incorrLow.write(i)\n",
    "        \n",
    "        word_counts.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sensitivity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f0c9308cd7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msensitivity_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msensitivity_sample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msensitivity_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# ---------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_params' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "#prefix = \"jeff go to supermarket and buy oranges for dinner: \"\n",
    "#prefix = \"we get on a taxi and go to times square in the morning: \"\n",
    "#prefix = \"we were in NYC last week and stay at Hilton.  The service there was great. : \"\n",
    "#prefix = \"we visited my mom and she was not there and we end up going to spa : \"\n",
    "prefix = \"%%%%%%% : \"\n",
    "\n",
    "\n",
    "prefix_tokens = word_tokenize(prefix.lower())\n",
    "\n",
    "# Function to add prefix to input features \n",
    "def add_prefix(features):   \n",
    "    new_features = ds.vocab.words_to_ids(utils.canonicalize_word(w, ds.vocab) for w in prefix_tokens)\n",
    "    new_features.extend(features)\n",
    "    return new_features[0:input_length]\n",
    "\n",
    "    \n",
    "sensitivity_batches = 100  \n",
    "sensitivity_sample_size = train_params['batch_size'] * sensitivity_batches\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# check the accuracy of the sample before adding the prefix \n",
    "# ---------------------------------------------------------\n",
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        ds.padded_test_features, ds.test_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "    if test_batches == sensitivity_batches:\n",
    "        break\n",
    "    \n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Sensitivity test accuracy before adding prefix:{}\".format(test_accuracy))\n",
    "\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# check the accuracy of the sample before adding the prefix \n",
    "# ---------------------------------------------------------\n",
    "test_batches = 1\n",
    "test_accuracy = 0.0\n",
    "test_pred_y = []\n",
    "\n",
    "# Add prefix to the user reviews\n",
    "\n",
    "sensitivity_features = []\n",
    "sensitivity_labels = ds.test_labels[:sensitivity_sample_size]\n",
    "for i in range(sensitivity_sample_size):\n",
    "    sensitivity_features.append(add_prefix(ds.padded_test_features[i]))\n",
    "\n",
    "for (tx, ty) in utils.multi_batch_generator(train_params['batch_size'], \\\n",
    "                                        sensitivity_features, sensitivity_labels):\n",
    "\n",
    "    batch_accuracy, pred_max = sess.run([accuracy_, pred_max_], feed_dict={X: tx, Y: ty})\n",
    "\n",
    "    test_batches +=1\n",
    "    test_accuracy += batch_accuracy\n",
    "    test_pred_y.append(pred_max.tolist())\n",
    "    \n",
    "\n",
    "test_accuracy = test_accuracy/test_batches\n",
    "print(\"Sensitivity test accuracy after adding prefix:{}\".format(test_accuracy))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Visualize the attributions on user reviewers with added prefix \n",
    "# --------------------------------------------------------------\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    integrated_gradients, pred_y, pred_proba, empty_pred_proba = \\\n",
    "                                get_integrated_gradients_new(sess, sess.graph, sensitivity_features[i])\n",
    "   \n",
    "    print(np.matrix(integrated_gradients).sum())\n",
    "    print(pred_proba[0][pred_y])\n",
    "    print(empty_pred_proba[0][pred_y])\n",
    "\n",
    "    pred_text = \"predicted = {}, actual = {}\".format(pred_y+2, sensitivity_labels[i]+2)\n",
    "    print(\"{}, {}\".format(i, pred_text))\n",
    "\n",
    "    #visualization.visualize_token_attrs(ds.vocab.ids_to_words(ds.test_features[i])[:input_length], np.matrix(gradients).sum(axis=1))\n",
    "    html_text = visualization.visualize_token_attrs(\n",
    "            ds.vocab.ids_to_words(sensitivity_features[i])[:input_length], \n",
    "            np.matrix(integrated_gradients).sum(axis=1), \n",
    "            pred_y+2, sensitivity_labels[i]+2)\n",
    "\n",
    "    text = \"</br>\" + pred_text + \"</br>\" + html_text\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
